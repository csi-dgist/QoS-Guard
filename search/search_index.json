{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"QoS Guard: ROS 2 QoS Static Validator <p> Dependency Chain Analysis of ROS 2 DDS QoS Policies: From Lifecycle Tutorial to Static Validation </p> <p> </p> Abstract <p>ROS 2 is built on the Data Distribution Service (DDS) and leverages more than 20 Quality of Service (QoS) policies to control communication availability, reliability, and resource usage. However, in practice, users often lack clear guidance or pre-verification procedures for combining these policies, which frequently forces them into trial-and-error tuning or results in unexpected runtime failures. To address this challenge, we decompose DDS publisher\u2013subscriber communication into three phases\u2014Discovery, Data Exchange, and Disassociation\u2014and provide a tutorial-style explanation of how 16 key QoS policies operate at each stage. We also systematically analyze inter-policy dependencies, deriving a QoS Dependency Chain, and classify 40 common constraints into a set of Dependency-Violation Rules. Building on this analysis, we developed the QoS Guard package, which enables offline verification of DDS XML profiles to detect potential conflicts before deployment. This allows users to safely configure QoS settings without needing to launch a ROS 2 session. By offering both conceptual insights and a practical tool, this work helps ROS 2 users better understand and manage QoS policies, ultimately improving the reliability of robot communications and the efficiency of resource utilization.</p>      QoS Guard Framework"},{"location":"QoS/","title":"DDS QoS Policy Guide","text":"<p>A comprehensive reference for configuring 16 key DDS QoS policies</p>"},{"location":"QoS/#16-qos-policies","title":"16 QoS Policies","text":"01ENTITY FACTORYENTFAC 02PARTITIONPART 03USER DATAUSRDATA 04GROUP DATAGRPDATA 05TOPIC DATATOPDATA 06RELIABILITYRELIAB 07DURABILITYDURABL 08DEADLINEDEADLN 09LIVELINESSLIVENS 10HISTORYHIST 11RESOURCE LIMITSRESLIM 12LIFESPANLFSPAN 13OWNERSHIPOWNST 14DESTINATION ORDERDESTORD 15WRITER DATA LIFECYCLEWDLIFE 16READER DATA LIFECYCLERDLIFE"},{"location":"QoS/#lifecycle-of-dds-communication","title":"Lifecycle of DDS Communication","text":"Three Phases: Discovery \u2192 Data Exchange \u2192 Disassociation.    1. Discovery Phase      Entities with the same topic are matched through PDP/EDP protocols and established after verifying QoS compatibility.    2. Data Exchange Phase      Matched pairs exchange user data and control metatraffic (HEARTBEAT/ACKNACK) to ensure reliability and timeliness.    3. Disassociation Phase      Communication ends by disposing instances or removing GUIDs, followed by purging all history after a timeout.    Lifecycle Mapping QoS Policy Discovery Data Exchange Disassociation ENTITY_FACTORY O PARTITION O USER_DATA O GROUP_DATA O TOPIC_DATA O RELIABILITY O O O DURABILITY O O O DEADLINE O O LIVELINESS O O O HISTORY O RESOURCE_LIMITS O LIFESPAN O OWNERSHIP O O O DESTINATION_ORDER O O WRITER_DATA_LIFECYCLE O READER_DATA_LIFECYCLE O"},{"location":"QoS/#1-entity-factory-entfac","title":"1. ENTITY FACTORY (ENTFAC)","text":"<p>Controls whether newly created DDS entities automatically start participating in discovery</p> Parameter <code>autoenable_created_entities</code> (default: TRUE) Mutability Can be changed at runtime"},{"location":"QoS/#mode","title":"Mode","text":"<ul> <li>TRUE: Newly created child entities are immediately enabled and begin participating in discovery.</li> <li>FALSE: The application must explicitly call enable() before the entity can participate in discovery.</li> </ul>"},{"location":"QoS/#example","title":"Example","text":"<p>The ENTFAC QoS can be used to conserve resources and allow multiple robots to initiate discovery simultaneously under synchronized conditions. For instance, the Publishers and Subscribers of a navigation module may be activated only after completing local sensor calibration or localization. By setting autoenable created entities=false, the system delays communication until the robot is ready by explicitly calling enable() at the appropriate time</p>"},{"location":"QoS/#2-partition-part","title":"2. PARTITION (PART)","text":"<p>Introduces logical segmentation with in a single DDS domain</p> Parameter <code>names</code> (default: emptystring) Mutability Can be changed at runtime"},{"location":"QoS/#example_1","title":"Example","text":"<p>The PART QoS can be used to separate identical data types into multiple logical groups without the need to define additional topics or create new domains. For instance, delivery robots and inventory robots may share common topics such as \u201cstatus\u201d and \u201ccommand\u201d within the same domain, but still require distinct data flows. By setting the names=delivery for the delivery robots and names=inventory for the inventory robots, a central management system can subscribe only to the desired partition and selectively receive data from a specific group of robots.</p>"},{"location":"QoS/#3-user-data-usrdata","title":"3. USER DATA (USRDATA)","text":"<p>Allows application-specific meta data to be attached to DDS entities such as Domain Participant, Publisher, and Subscriber</p> Parameter <code>value</code> (default: empty sequence) Mutability Can be changed at runtime"},{"location":"QoS/#example_2","title":"Example","text":"<p>The USRDATA QoS can be used to flexibly deliver iden tity, authentication, and configuration information without re quiring additional topics or separate domains. For example, each robot may embed value such as robot id=R12 and to ken=ABCD123 in its participant, allowing the server to inspect the token during the SPDP phase and admit only authorized robots while blocking others. Similarly, a Publisher for a LiDAR topic may include value such as sensor=LiDAR and fov=270 in its USRDATA, enabling the subscribing application to determine sensor configuration and immediately select an appropriate filtering strategy before receiving any samples.</p>"},{"location":"QoS/#4-group-data-grpdata","title":"4. GROUP DATA (GRPDATA)","text":"<p>Attaches application-specific metadata to the Publisher and Subscriber entities</p> Parameter <code>value</code> (default: empty sequence) Mutability Can be changed at runtime"},{"location":"QoS/#example_3","title":"Example","text":"<p>The GRPDATA QoS can be used to logically segment data f lows in a manner similar to the PART QoS. For example, if delivery and inventory robots share topics within the same domain, assigning value=delivery or value=inventory to each Publisher or Subscriber allows the central management server to read these values during discovery callbacks. Although similar to PART, the key distinction is that PART enforces matching at the DDS level, whereas GRPDATA leaves the interpretation of the field entirely to the application.</p>"},{"location":"QoS/#5-topic-data-topdata","title":"5. TOPIC DATA (TOPDATA)","text":"<p>Attaches application-specific metadata to the topic entity</p> Parameter <code>value</code> (default: empty sequence) Mutability Can be changed at runtime"},{"location":"QoS/#example_4","title":"Example","text":"<p>The TOPDATA QoS can be used by applications to verify schema compatibility in advance. For example, each robot can embed value such as schema=2.1 and frame=lidar in the TOPDATA of the scan cloud topic. During topic discovery, an inventory management application can read this information, and if the schema is incompatible, it can prevent subscription and avoid data parsing errors.</p>"},{"location":"QoS/#6-reliability-reliab","title":"6. RELIABILITY (RELIAB)","text":"<p>Determines whether entities such as topic, Publisher, and Subscriber transmit data reliably</p> Parameter1 <code>kind</code> (default: Publisher-reliable, Subscriber-best_effrot) Parameter2 <code>max blocking time</code> Mutability Can not be changed at runtime"},{"location":"QoS/#kind-mode","title":"kind Mode","text":"<ul> <li>RELIABLE: Attempts to deliver all samples stored in the Publisher HistoryCache to the Subscriber.Retransmissions occur upon request using ACK/NACK signaling. (max blocking time limits how long a Publisher\u2019s write() or dispose() operation can be blocked due to delayed ACKs or buffer unavailability.)</li> <li>BEST_EFFORT: Publisher does not wait for ACKs and does not retransmit lost samples.</li> </ul>"},{"location":"QoS/#example_5","title":"Example","text":"<p>The RELIAB QoS can be used to balance safety and efficiency by configuring topics according to their criticality. For commands such as emergency stops or task assignments, both Publisher and Subscriber should use reliable, ensuring guaranteed delivery. In contrast, high-frequency data such as LiDAR scans or camera streams can use best effort, which avoids retransmission overhead and tolerates occasional loss. </p>"},{"location":"QoS/#7-durability-durabl","title":"7. DURABILITY (DURABL)","text":"<p>Determines how a late-joining Subscriber can receive previously published samples from a Publisher</p> Parameter <code>kind</code> (default: VOLATILE) Mutability Can not be changed at runtime"},{"location":"QoS/#kind-mode_1","title":"kind Mode","text":"<ul> <li>VOLATILE: Does not send any previous samples to newly joined Subscribers.</li> <li>TRANSIENT_LOCAL: Retains samples in the Publisher\u2019s HistoryCache while it is active, allowing late-joining Subscribers to access previously published data.</li> <li>TRANSIENT: Preserve data after a Publisher has been terminated. Retains data in volatile memory.</li> <li>PERSISTENT: Preserve data after a Publisher has been terminated. Uses non-volatile storage such as files or databases</li> </ul>"},{"location":"QoS/#example_6","title":"Example","text":"<p>The DURABL QoS can be used to allow newly added or recovered robots to immediately access the same information, thereby improving both system robustness and collaborative efficiency. Data such as global maps or mission plans, whose availability must not depend on the lifespan of a specific robot, should remain accessible to robots that join after the Publisher has terminated or rebooted. In such cases, DURABL should be set to transient if data must persist throughout process restarts, or to persistent if it must survive system-wide reboots.</p>"},{"location":"QoS/#8-deadline-deadln","title":"8. DEADLINE (DEADLN)","text":"<p>Specifies the maximum interval within which a new sample for a given data instance must be produced by the Publisher and received by the Subscriber</p> Parameter <code>period</code> (default: infinity) Mutability Can be changed at runtime"},{"location":"QoS/#example_7","title":"Example","text":"<p>The DEADLN QoS can be used for real-time monitoring of robot status. Each robot may publish its position and battery level through ROS 2 topics every second. The Publisher and the central monitoring system\u2019s Subscriber are both configured with a period of 1 second. If a new sample fails to arrive within this interval, the monitoring system receives a deadline-miss notification, enabling immediate detection of communication failures or faults in that data stream. The application can then respond by issuing alerts or stopping the robot, thereby enhancing overall system safety and reliability.</p>"},{"location":"QoS/#9-liveliness-livens","title":"9. LIVELINESS (LIVENS)","text":"<p>Determine whether its corresponding Publisher is still active</p> Parameter1 <code>kind</code> (default: AUTOMATIC) Parameter2 <code>lease_duration (default: infinite)</code> Mutability Can not be changed at runtime"},{"location":"QoS/#kind-mode_2","title":"kind Mode","text":"<ul> <li>AUTOMATIC: The DomainParticipant asserts liveliness implicitly by periodical liveliness assertions.</li> <li>MANUAL_BY_PARTICIPANT: A single assertion from any entity within a DomainParticipant marks all of its Publishers as alive.</li> <li>MANUAL_BY_TOPIC: Each Publisher must explicitly assert its own liveliness by publishing HEARTBEAT samples or calling assert liveliness().</li> </ul>"},{"location":"QoS/#example_8","title":"Example","text":"<p>The LIVENS QoS can be used to verify whether the pub lishing process itself is still active, whereas the DEADLN QoS ensures the timely delivery of individual data samples. This policy enables a central monitoring system to automatically track the operational status of each robot. For instance, each robot may publish position and battery level via a Publisher. The central Subscriber is configured with kind set to automatic and lease duration set to 5 seconds, causing DDS to refresh liveliness every five seconds. If a signal is not received within the lease duration, a liveliness notification is triggered to indicate that the robot is inactive. The monitoring application can then respond by graying out the robot\u2019s icon on the map or issuing a warning.</p>"},{"location":"QoS/#10-history-hist","title":"10. HISTORY (HIST)","text":"<p>Determines how many samples a Publisher retains in its HistoryCache for retransmission, and how many samples a Subscriber stores before they are delivered to the application.</p> Parameter <code>kind</code> (default: KEEP_LAST, depth=1) Mutability Can not be changed at runtime"},{"location":"QoS/#kind-mode_3","title":"kind Mode","text":"<ul> <li>KEEP_LAST: Retains only the most recent samples per instance, with depth specifying the maximum number of samples to keep.</li> <li>KEEP_ALL: Stores all samples for each instance and attempts to deliver as many as possible.</li> </ul>"},{"location":"QoS/#example_9","title":"Example","text":"<p>The HIST QoS can be used to control how much robot data is retained. If a control station must preserve all robot positions since startup, kind=keep all can be set so that both the Publisher and the Subscriber store every sample. In contrast, for real-time tracking where only the latest position matters, setting kind=keep last with depth=1 ensures that each robot\u2019s Publisher retains and transmits only the most recent position, while the Subscriber receives only that single value.</p>"},{"location":"QoS/#11-resource-limits-reslim","title":"11. RESOURCE LIMITS (RESLIM)","text":"<p>Defines upper bounds on the number of instances and samples that the topic, Publishe, and Subscriber entities can manage</p> Parameter1 <code>max_samples</code> Parameter2 <code>max_instances</code> Parameter3 <code>max samples per instance</code> Mutability Can not be changed at runtime"},{"location":"QoS/#example_10","title":"Example","text":"<p>The RESLIM QoS can be used to manage resources and maintain communication stability. For instance, where only the most recent information is important and historical records are less critical, such as a robot\u2019s real-time position, the Publisher\u2019s max instances can be set to a small value to avoid excessive memory usage. On the Subscriber side, where the number of participating robots may be large or variable, the max samples per instance value should be set high enough to keep minimal data for each robot instance.</p>"},{"location":"QoS/#12-lifespan-lfspan","title":"12. LIFESPAN (LFSPAN)","text":"<p>How long a sample published by a Publisher remains valid</p> Parameter <code>duration</code> (default: infinity) Mutability Can be changed at runtime"},{"location":"QoS/#example_11","title":"Example","text":"<p>The LFSPAN QoS can be used to prevent robots from retaining outdated samples unnecessarily. For data such as po sition or battery level, where only the last few seconds matter, setting the duration accordingly ensures that the Publisher\u2019s HistoryCache stores only the most recent samples, with older ones automatically deleted to conserve memory. In contrast, for data such as command logs\u2013 where long-term delivery is more important than freshness\u2013 it is preferable to set duration to infinity and rely on other QoS policies to ensure reliability.</p>"},{"location":"QoS/#13-ownership-strength-ownst","title":"13. OWNERSHIP (+STRENGTH) (OWNST)","text":"<p>Whether multiple Publishers can concurrently update the same instance, or, if not, which Publisher\u2019s value should be accepted</p> Parameter <code>kind</code> (default: SHARED) Mutability Can not be changed at runtime"},{"location":"QoS/#kind-mode_4","title":"kind Mode","text":"<ul> <li>SHARED: Allows multiple Publishers to freely update the same instance.</li> <li>EXCLUSIVE: Only a single Publisher is allowed to update an instance, and its updates alone are delivered to Subscribers.Priority is determined by the value of the OWNST STRENGTH QoS policy.</li> </ul>"},{"location":"QoS/#example_12","title":"Example","text":"<p>The OWNSTandOWNERSHIP STRENGTHQoSpolicies can be used to manage shared resources or mission states con sistently. For example, if multiple robots scan the environment simultaneously to build a shared map, shared mode allows the server to receive updates from all robots and generate a unified map. In contrast, for tasks that must be performed by a single robot, exclusive mode can be used with appropriate value assigned to each Publisher, ensuring that the active robot becomes the instance owner. If the current owner robot fails to respond due to a fault, DDS automatically transfers ownership to the standby robot with the next highest value, allowing the task to continue without interruption.</p>"},{"location":"QoS/#14-destination-order-destord","title":"14. DESTINATION ORDER (DESTORD)","text":"<p>How a Subscriber orders samples from multiple Publishers targeting the same instance</p> Parameter <code>kind</code> (default: BY_RECEPTION_TIMESTAMP) Mutability Can not be changed at runtime"},{"location":"QoS/#mode_1","title":"Mode","text":"<ul> <li>BY_RECEPTION_TIMESTAMP: Orders samples by their reception time at the Subscriber.</li> <li>BY_SOURCE_TIMESTAMP: Uses the timestamp assigned by the Publisher at publication, preserving the original creation order regardless of delays.</li> </ul>"},{"location":"QoS/#example_13","title":"Example","text":"<p>The DESTORD QoS policy can be used to maintain data consistency when multiple robots simultaneously update the same instance. By configuring by source timestamp, samples are ordered by their creation time regardless of network delays or arrival order, allowing all robots to share a consistent view of the map. In contrast, for real-time data such as current posi tions, where the latest received value is most important, using by reception timestamp enables the Subscriber to reflect the earliest arriving sample immediately.</p>"},{"location":"QoS/#15-writer-data-lifecycle-wdlife","title":"15. WRITER DATA LIFECYCLE (WDLIFE)","text":"<p>whether a Publisher should notify Subscribers with a dispose() when it unregisters an instance</p> Parameter <code>autodispose unregistered instances</code> (default: TRUE) Mutability Can be changed at runtime"},{"location":"QoS/#mode_2","title":"Mode","text":"<ul> <li>TRUE: An instance to be automatically marked as disposed when an unregister() is called, so that it is recognized as deleted on the Subscriber side.</li> <li>FALSE: The unregister() only disassociates the writer from the instance without disposing of it; the application must explicitly call dispose() to fully delete the instance.</li> </ul>"},{"location":"QoS/#example_14","title":"Example","text":"<p>The WDLIFE QoS can be used to explicitly manage the lifecycle of object-based tasks. For example, when a robot detects an object with its sensors, it can publish the object\u2019s key, position, type, and status on a topic, allowing other robots or the control system to subscribe and build a shared environment model. When the task is completed, the post processing behavior depends on the detecting robot\u2019s autodis pose unregistered instances setting. If true, the instance is immediately disposed of and removed from the map or marked as \u201cprocessed\u201d; if false, only the Publisher-instance link is removed while the object information remains active, allowing another robot to rediscover and update the same object, thereby improving collaboration flexibility.</p>"},{"location":"QoS/#16-reader-data-lifecycle-rdlife","title":"16. READER DATA LIFECYCLE (RDLIFE)","text":"<p>How long a Subscriber retains samples that have been disposed of or are no longer associated with any Publisher</p> Parameter1 <code>autopurge disposed samples delay</code> (default: infinity) Parameter2 <code>autopurge no writer samples delay</code> (default: infinity) Mutability Can be changed at runtime"},{"location":"QoS/#example_15","title":"Example","text":"<p>The RDLIFE QoS policy enables tiered instance man agement for improved efficiency. For example, in a tempo rary storage area where hundreds of pallets move quickly and historical positions become obsolete immediately, set ting autopurge disposed samples delay to 0 seconds ensures that the cache is cleared as soon as a robot calls dis pose(). This keeps the cache lightweight and prevents un necessary memory growth. In contrast, for static, critical objects awaiting inspection after relocation, setting autop urge no writer samples delay to 300 seconds allows newly joined robots to continue inspection even after brief com munication interruptions. By adjusting the reader-side delay values based on context, essential data can be preserved while irrelevant information is promptly discarded, ensuring efficient use of system resources.</p>"},{"location":"QoS/#summary-metadata-matching-cache-and-lifecycle","title":"Summary: Metadata, Matching, Cache, and Lifecycle","text":"Category QoS One-line summary Discovery timing ENTFAC When entities participate in discovery Logical segmentation PART Partition names to separate or group topic flows Metadata USRDATA, GRPDATA, TOPDATA Application info on Participant / Pub-Sub / Topic Delivery guarantee RELIAB best_effort vs reliable (retransmission, ordering) Late joiners DURABL How much past data late joiners can receive Temporal constraints DEADLN, LIVENS Period (deadline) monitoring; Publisher liveness Cache HIST, RESLIM, LFSPAN How many samples, upper bounds, validity duration Multiple writers OWNST, DESTORD Single owner vs shared; order by reception vs source timestamp Instance cleanup WDLIFE, RDLIFE Dispose on unregister; when to purge disposed/no-writer samples"},{"location":"QoS_Guard/","title":"QoS-Guard","text":"<p>Offline static validation of DDS QoS for ROS 2. </p> <p>Find QoS mismatches and dependency violations before you run your nodes\u2014no ROS 2 runtime required.</p>"},{"location":"QoS_Guard/#overview","title":"Overview","text":"<p>ROS 2 uses DDS and QoS policies (reliability, durability, history, etc.) for topic communication. </p> <p>If Publisher and Subscriber QoS do not match or conflict, you can get failed connections, data loss, or crashes.</p> <p>QoS-Guard scans your XML profiles and source code (e.g. <code>rclcpp::QoS</code>, <code>create_publisher</code>), builds Pub\u2013Sub pairs per topic, and runs 40 dependency rules to report potential issues.</p> \u26a1 No runtime No need to run <code>ros2 run</code> or any node \ud83d\udcbb Python 3.10+ Works with or without ROS 2 installed \ud83d\udcc2 Package mode Point to a package path \u2192 auto-scan XML + code, verify all topic pairs \ud83d\udcc4 XML pair mode Point to one pub XML + one sub XML \u2192 verify that pair only (Fast/Connext)"},{"location":"QoS_Guard/#quick-start","title":"Quick Start","text":"<p>After installing as ROS 2 package (see Installation)</p> <pre><code>qos_guard /path/to/your_ros2_package\n</code></pre> <p>Specify DDS and ROS version</p> <pre><code>qos_guard /path/to/package fast jazzy\n</code></pre> <p>Without ROS 2: run from repo root</p> <pre><code>cd /path/to/QoS-Guard\npython3 -m qos_guard.qos_checker /path/to/package\n</code></pre>"},{"location":"QoS_Guard/#requirements","title":"Requirements","text":"Python 3.10 or higher ROS 2 Optional (Humble, Jazzy, or Kilted) OS Linux recommended / Windows (Python only)"},{"location":"QoS_Guard/#installation","title":"Installation","text":""},{"location":"QoS_Guard/#option-a-as-a-ros-2-package-recommended-if-you-use-ros-2","title":"Option A: As a ROS 2 package (recommended if you use ROS 2)","text":"<p>After this, you run the tool with <code>ros2 run qos_guard qos_guard ...</code>.</p> <p>1. Clone into your workspace src <pre><code>mkdir -p ~/ros2_ws/src\ncd ~/ros2_ws/src\ngit clone &lt;repository_URL&gt; qos-guard\n</code></pre></p> <p>2. Build and source <pre><code>cd ~/ros2_ws\ncolcon build --packages-select qos_guard\nsource install/setup.bash\n</code></pre></p> <p>3. Run <pre><code>ros2 run qos_guard qos_guard /path/to/any/ros2/package\n</code></pre></p>"},{"location":"QoS_Guard/#option-b-standalone-no-ros-2","title":"Option B: Standalone (no ROS 2)","text":"<p>You only need the repository and Python 3.10+.</p> <p>1. Clone and go to repo root <pre><code>cd /path/to/QoS-Guard\n</code></pre></p> <p>2. Run <pre><code>python3 -m qos_guard.qos_checker /path/to/your_ros2_package\n</code></pre></p>"},{"location":"QoS_Guard/#usage","title":"Usage","text":"<p>The tool has three modes. Examples below use the <code>qos_guard</code> command.</p>"},{"location":"QoS_Guard/#1-package-mode-default","title":"1. Package mode (default)","text":"<p>When to use  Check a whole ROS 2 package. The tool finds all <code>*.xml</code> (except <code>package.xml</code>) and scans <code>.cpp</code>, <code>.hpp</code>, <code>.h</code>, <code>.py</code> for publishers/subscribers, builds Pub\u2013Sub pairs by topic, and runs the rule checks.</p> <pre><code>qos_guard &lt;package_path&gt; [dds] [ros_version] [publish_period=&lt;N&gt;ms] [rtt=&lt;N&gt;ms]\n</code></pre> <p>Examples</p> <pre><code># Defaults: DDS=fast, ROS=humble, publish_period=40ms, rtt=50ms\nqos_guard ~/ros2_ws/src/my_robot_pkg\n\n# Fast DDS + Jazzy\nqos_guard ~/ros2_ws/src/my_robot_pkg fast jazzy\n\n# Custom period and RTT \nqos_guard ~/ros2_ws/src/my_robot_pkg fast humble publish_period=20ms rtt=30ms\n</code></pre>"},{"location":"QoS_Guard/#2-xml-pair-mode","title":"2. XML pair mode","text":"<p>When to use  You have one Writer QoS XML and one Reader QoS XML and want to verify only that pair. Supported for Fast DDS and Connext only.</p> <pre><code>qos_guard --xml &lt;pub.xml&gt; &lt;sub.xml&gt; &lt;dds&gt; &lt;ros_version&gt; [publish_period=&lt;N&gt;ms] [rtt=&lt;N&gt;ms]\n</code></pre> <p>Examples</p> <pre><code>qos_guard --xml ./profiles/writer.xml ./profiles/reader.xml fast humble\nqos_guard -x pub_qos.xml sub_qos.xml connext jazzy publish_period=10ms\n</code></pre> <p>Cyclone DDS does not support XML QoS profiles. For Cyclone, use package mode only (code scan). The <code>--xml</code> option is not available when <code>dds=cyclone</code>.</p>"},{"location":"QoS_Guard/#3-list-mode","title":"3. List mode","text":"<p>When to use See which XML files the tool would scan under a package (useful to confirm config layout).</p> <pre><code>qos_guard --list &lt;package_path&gt;\n</code></pre> <p>Example output</p> <pre><code>Found 4 XML file(s) in /path/to/my_pkg\n  /path/to/my_pkg/config/fastdds.xml\n  /path/to/my_pkg/profiles/topic_profiles.xml\n  ...\n</code></pre>"},{"location":"QoS_Guard/#arguments-summary","title":"Arguments summary","text":"package_path Path to the ROS 2 package directory \u00b7 Default: Required in package/list mode dds <code>fast</code>, <code>cyclone</code>, or <code>connext</code> \u00b7 Default: <code>fast</code> ros_version <code>humble</code>, <code>jazzy</code>, or <code>kilted</code> \u00b7 Default: <code>humble</code> publish_period=&lt;N&gt;ms Writer publish period (ms); used by rules that depend on period \u00b7 Default: <code>40ms</code> rtt=&lt;N&gt;ms Expected round-trip time (ms); used by reliability/depth rules \u00b7 Default: <code>50ms</code>"},{"location":"QoS_Guard/#dds-support","title":"DDS support","text":"DDS XML profiles Source code scan Fast DDS \u2713 \u2713 RTI Connext \u2713 \u2713 Cyclone DDS \u2014 \u2713"},{"location":"QoS_Guard/#how-qos-is-applied-package-mode","title":"How QoS is applied (package mode)","text":"<p>When both XML and code define QoS, the highest priority source wins (and overrides lower ones):</p> L1 Code (<code>rclcpp::QoS</code>, etc.)Non-default values in code override XML. L2 <code>&lt;topic profile_name=\"/topic_name\"&gt;</code>Matched by topic name; works across ROS 2 versions. L3 <code>&lt;data_writer&gt;</code> / <code>&lt;data_reader&gt;</code>Jazzy/Kilted style; topic name matching. L4 <code>&lt;publisher&gt;</code> / <code>&lt;subscriber&gt;</code>Humble style; profile name from code. L5 <code>is_default_profile=\"true\"</code>Fallback when nothing else matches. <p>Pub\u2013Sub pairing in package mode: same topic name \u2192 same pair. If there is no topic name, the tool derives a base name from <code>profile_name</code> (e.g. <code>cmd_vel_pub</code> and <code>cmd_vel_subscriber</code> \u2192 base <code>cmd_vel</code>) and pairs by that.</p>"},{"location":"QoS_Guard/#verification-results","title":"Verification results","text":"<p>The tool reports violations by severity. Output is color-coded in the terminal; a summary table at the end shows counts per category.</p> Structural RMW-level incompatibility; connection can fail or process may crash. Fix first; these block communication. Functional Connection may work but guarantees (reliability, durability, etc.) are broken. Risk of data loss or late-joiner issues; fix when possible. Operational No functional bug but inefficient (e.g. extra memory or bandwidth). Optional to fix; improves resource use. <p>If there are no violations, you see: <code>All Entities are safe !</code></p>"},{"location":"QoS_Guard/#test-package","title":"Test package","text":"<p>The repo includes qos_test_pkg to try the tool: multiple topics, mixed QoS styles (code-only, XML topic profile, entity profile, default trap).</p> <pre><code>cd ~/ros2_ws\ncolcon build --packages-select qos_test_pkg\nsource install/setup.bash\n\nqos_guard ~/ros2_ws/src/qos_test_pkg fast humble\n</code></pre> <p>For topic layout and profile descriptions, see <code>qos_test_pkg/README.md</code>.</p>"},{"location":"QoS_Guard/#fast-dds-external-xml-package-mode","title":"Fast DDS: external XML (package mode)","text":"<p>When <code>dds=fast</code>, package mode can also load XML from these environment variables (so system-wide or workspace QoS is included):</p> Variable Purpose <code>FASTRTPS_DEFAULT_PROFILES_FILE</code> Fast DDS default profiles XML path. <code>RMW_FASTRTPS_CONFIG_FILE</code> ROS 2 rmw_fastrtps config XML path. <p>Example:</p> <pre><code>export FASTRTPS_DEFAULT_PROFILES_FILE=/path/to/default_profiles.xml\nqos_guard /path/to/package fast humble\n</code></pre> <p>Or one-off:</p> <pre><code>FASTRTPS_DEFAULT_PROFILES_FILE=/path/to/default_profiles.xml qos_guard /path/to/package fast humble\n</code></pre>"},{"location":"QoS_Guard/#faq","title":"FAQ","text":"Can I use it without ROS 2? Yes. Use <code>python3 -m qos_guard.qos_checker &lt;package_path&gt; [dds] [ros_version]</code> from the repo root. Why does <code>--xml</code> not work with Cyclone? Cyclone DDS has no XML QoS profile support; only package mode (code scan) is available. How do I check only one topic? Use XML pair mode with that topic\u2019s pub/sub XML files, or a minimal package containing only that topic\u2019s config/code. How do I see actual QoS at runtime? Use <code>ros2 topic echo /topic_name --qos-profile all</code>. QoS-Guard is for static checks before deployment. Exit code is 0 but violations were printed. The tool may still exit 0 when violations exist; use the printed report and summary table to see if anything failed."},{"location":"XML_Generator/","title":"DDS QoS XML Generator","text":"<p>A tool for automatically generating XML files for DDS QoS test cases. It produces efficient test cases using Pairwise testing, covering combinations of 16 parameters with minimal redundancy.</p>"},{"location":"XML_Generator/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Parameter List</li> <li>Pairwise Test Case Generation</li> <li>Final Case Count</li> <li>How to Run</li> <li>Reference</li> </ul>"},{"location":"XML_Generator/#download","title":"Download","text":"<p>Download XML Generator Package (ZIP)</p>"},{"location":"XML_Generator/#parameter-list","title":"Parameter List","text":""},{"location":"XML_Generator/#overview","title":"Overview","text":"Parameter Pub Values Pub Value Types Sub Values Sub Value Types Constraint Combinations ENTITY_FACTORY 2 True, False 2 True, False pub=sub 2 DATA_EXISTS 2 exists, not_exists 2 exists, not_exists pub=sub 2 RELIABILITY 2 RELIABLE, BEST_EFFORT 2 RELIABLE, BEST_EFFORT Independent 4 DURABILITY 4 TRANSIENT_LOCAL, VOLATILE, TRANSIENT, PERSISTENT 4 TRANSIENT_LOCAL, VOLATILE, TRANSIENT, PERSISTENT Independent 16 DEADLINE 3 PP, 2\u00d7PP, DURATION_INFINITY 3 PP, 2\u00d7PP, DURATION_INFINITY Independent 9 LIVELINESS 6 (AUTOMATIC, 0.5\u00d7PP), (AUTOMATIC, PP), (AUTOMATIC, 2\u00d7PP), (AUTOMATIC, \u221e), (MANUAL_BY_PARTICIPANT, None), (MANUAL_BY_TOPIC, None) 6 (same) Independent 36 HISTORY 5 (KEEP_ALL, None), (KEEP_LAST, 1), (KEEP_LAST, &lt;(RTT/PP)+2), (KEEP_LAST, =(RTT/PP)+2), (KEEP_LAST, &gt;(RTT/PP)+2) 5 (same) Independent 25 RESOURCE_LIMITS_MAX_SAMPLES_PER_INSTANCE 4 1, &lt;(RTT/PP)+2, =(RTT/PP)+2, &gt;(RTT/PP)+2 4 (same) Independent 16 RESOURCE_LIMITS_MAX_SAMPLES 2 1, (RTT/PP+3)\u00d7PP 2 (same) Independent 4 LIFESPAN 2 0.5\u00d7RTT, DURATION_INFINITY 2 (same) Independent 4 OWNERSHIP 2 SHARED, EXCLUSIVE 2 (same) Independent 4 DESTINATION_ORDER 2 BY_RECEPTION_TIMESTAMP, BY_SOURCE_TIMESTAMP 2 (same) Independent 4 WRITER_DATA_LIFECYCLE 2 True, False 0 N/A (pub only) pub only 2 READER_DATA_LIFECYCLE_NO_WRITER 0 N/A (sub only) 2 0, 3 sub only 2 READER_DATA_LIFECYCLE_DISPOSED 0 N/A (sub only) 2 0, 3 sub only 2"},{"location":"XML_Generator/#parameter-details","title":"Parameter Details","text":""},{"location":"XML_Generator/#1-entity_factory","title":"1. ENTITY_FACTORY","text":"<ul> <li>Constraint: pub and sub must always have the same value.</li> <li>Combinations: (True, True), (False, False)</li> </ul>"},{"location":"XML_Generator/#2-data_exists","title":"2. DATA_EXISTS","text":"<ul> <li>Description: Combined parameter for PARTITION, USER_DATA, GROUP_DATA, TOPIC_DATA.</li> <li>Constraint: pub and sub must always have the same value.</li> <li>Combinations: (exists, exists), (not_exists, not_exists)</li> </ul>"},{"location":"XML_Generator/#3-reliability","title":"3. RELIABILITY","text":"<ul> <li>Independent: pub and sub can be set independently.</li> <li>Combinations: 2 \u00d7 2 = 4</li> </ul>"},{"location":"XML_Generator/#4-durability","title":"4. DURABILITY","text":"<ul> <li>Independent: pub and sub can be set independently.</li> <li>Combinations: 4 \u00d7 4 = 16</li> </ul>"},{"location":"XML_Generator/#5-deadline","title":"5. DEADLINE","text":"<ul> <li>Values:</li> <li><code>PP</code>: D &lt; 2\u00d7PP case</li> <li><code>2\u00d7PP</code>: D \u2265 2\u00d7PP case</li> <li><code>DURATION_INFINITY</code>: infinite</li> <li>Independent: pub and sub can be set independently.</li> <li>Combinations: 3 \u00d7 3 = 9</li> </ul>"},{"location":"XML_Generator/#6-liveliness","title":"6. LIVELINESS","text":"<ul> <li>Values:</li> <li><code>(AUTOMATIC, 0.5\u00d7PP)</code>: lease_duration &lt; 2\u00d7PP</li> <li><code>(AUTOMATIC, PP)</code>: lease_duration &lt; 2\u00d7PP</li> <li><code>(AUTOMATIC, 2\u00d7PP)</code>: lease_duration \u2265 2\u00d7PP</li> <li><code>(AUTOMATIC, DURATION_INFINITY)</code>: infinite</li> <li><code>(MANUAL_BY_PARTICIPANT, None)</code>: manual type</li> <li><code>(MANUAL_BY_TOPIC, None)</code>: manual type</li> <li>Independent: pub and sub can be set independently.</li> <li>Combinations: 6 \u00d7 6 = 36</li> </ul>"},{"location":"XML_Generator/#7-history","title":"7. HISTORY","text":"<ul> <li>Values:</li> <li><code>(KEEP_ALL, None)</code>: KEEP_ALL</li> <li><code>(KEEP_LAST, 1)</code>: depth=1</li> <li><code>(KEEP_LAST, &amp;lt;(RTT/PP)+2)</code>: depth &lt; (RTT/PP)+2</li> <li><code>(KEEP_LAST, =(RTT/PP)+2)</code>: depth = (RTT/PP)+2</li> <li><code>(KEEP_LAST, &amp;gt;(RTT/PP)+2)</code>: depth &gt; (RTT/PP)+2</li> <li>Independent: pub and sub can be set independently.</li> <li>Combinations: 5 \u00d7 5 = 25</li> </ul>"},{"location":"XML_Generator/#8-resource_limits_max_samples_per_instance","title":"8. RESOURCE_LIMITS_MAX_SAMPLES_PER_INSTANCE","text":"<ul> <li>Values: <code>1</code>, <code>&amp;lt;(RTT/PP)+2</code>, <code>=(RTT/PP)+2</code>, <code>&amp;gt;(RTT/PP)+2</code></li> <li>Independent: pub and sub can be set independently.</li> <li>Combinations: 4 \u00d7 4 = 16</li> </ul>"},{"location":"XML_Generator/#9-resource_limits_max_samples","title":"9. RESOURCE_LIMITS_MAX_SAMPLES","text":"<ul> <li>Values: <code>1</code>, <code>(RTT/PP+3)\u00d7PP</code></li> <li>Independent: pub and sub can be set independently.</li> <li>Combinations: 2 \u00d7 2 = 4</li> </ul>"},{"location":"XML_Generator/#10-lifespan","title":"10. LIFESPAN","text":"<ul> <li>Values: <code>0.5\u00d7RTT</code>, <code>DURATION_INFINITY</code></li> <li>Independent: pub and sub can be set independently.</li> <li>Combinations: 2 \u00d7 2 = 4</li> </ul>"},{"location":"XML_Generator/#11-ownership","title":"11. OWNERSHIP","text":"<ul> <li>Values: <code>SHARED</code>, <code>EXCLUSIVE</code></li> <li>Independent: pub and sub can be set independently.</li> <li>Combinations: 2 \u00d7 2 = 4</li> </ul>"},{"location":"XML_Generator/#12-destination_order","title":"12. DESTINATION_ORDER","text":"<ul> <li>Values: <code>BY_RECEPTION_TIMESTAMP</code>, <code>BY_SOURCE_TIMESTAMP</code></li> <li>Independent: pub and sub can be set independently.</li> <li>Combinations: 2 \u00d7 2 = 4</li> </ul>"},{"location":"XML_Generator/#13-writer_data_lifecycle","title":"13. WRITER_DATA_LIFECYCLE","text":"<ul> <li>Description: Applied to Publisher only.</li> <li>Values: <code>True</code>, <code>False</code></li> <li>Combinations: 2</li> </ul>"},{"location":"XML_Generator/#14-reader_data_lifecycle_no_writer","title":"14. READER_DATA_LIFECYCLE_NO_WRITER","text":"<ul> <li>Description: Applied to Subscriber only.</li> <li>Values: <code>0</code>, <code>3</code></li> <li>Combinations: 2</li> </ul>"},{"location":"XML_Generator/#15-reader_data_lifecycle_disposed","title":"15. READER_DATA_LIFECYCLE_DISPOSED","text":"<ul> <li>Description: Applied to Subscriber only.</li> <li>Values: <code>0</code>, <code>3</code></li> <li>Constraint: Must match READER_DATA_LIFECYCLE_NO_WRITER value.</li> <li>Combinations: 2</li> </ul>"},{"location":"XML_Generator/#pairwise-test-case-generation","title":"Pairwise Test Case Generation","text":""},{"location":"XML_Generator/#what-is-pairwise-testing","title":"What is Pairwise Testing?","text":"<p>Pairwise testing generates a minimal set of test cases that cover every pair of parameter values at least once. This reduces the total number of cases while maintaining good coverage.</p>"},{"location":"XML_Generator/#generation-process","title":"Generation Process","text":"<ol> <li>Collect parameters</li> <li> <p>Select only parameters that have pub or sub values among the 16 parameters.</p> </li> <li> <p>Generate combinations per parameter</p> </li> <li><code>_get_param_values()</code> builds pub/sub combinations for each parameter.</li> <li> <p>Combination count is determined by constraints.</p> </li> <li> <p>Apply Pairwise algorithm</p> </li> <li>With allpairspy (optional): Uses an optimized algorithm.</li> <li> <p>Without allpairspy: Built-in simple Pairwise implementation:</p> <ul> <li>Generate all parameter pairs: C(16,2) = 120 pairs</li> <li>Build value combinations for each pair</li> <li>Create test cases (default values + current pair values)</li> <li>Remove duplicates</li> </ul> </li> <li> <p>Apply constraints</p> </li> <li>READER_DATA_LIFECYCLE_NO_WRITER and READER_DATA_LIFECYCLE_DISPOSED must have the same value.</li> <li>Exclude combinations that violate this.</li> </ol>"},{"location":"XML_Generator/#example","title":"Example","text":"<p>Parameters A, B, C with 2, 3, and 2 values respectively:</p> <ul> <li>Full combinations: 2 \u00d7 3 \u00d7 2 = 12</li> <li>Pairwise cases: Minimal set covering all pairs (A\u2013B, A\u2013C, B\u2013C) at least once (about 6\u20138 cases)</li> </ul> <p>This yields effective test coverage with far fewer cases than exhaustive combination.</p>"},{"location":"XML_Generator/#final-case-count","title":"Final Case Count","text":""},{"location":"XML_Generator/#theoretical-full-combination-count","title":"Theoretical Full Combination Count","text":"<p>If all parameters were combined independently:</p> <pre><code>2 \u00d7 2 \u00d7 4 \u00d7 16 \u00d7 9 \u00d7 36 \u00d7 25 \u00d7 16 \u00d7 4 \u00d7 4 \u00d7 4 \u00d7 4 \u00d7 2 \u00d7 2 \u00d7 2\n\u2248 271 trillion\n</code></pre>"},{"location":"XML_Generator/#actual-generated-case-count","title":"Actual Generated Case Count","text":"<p>With the Pairwise algorithm:</p> <ul> <li>Generated cases: about 853 (for PP=0.1, RTT=0.2)</li> <li>Efficiency: about 0.0000003% of full combination</li> </ul>"},{"location":"XML_Generator/#how-the-count-is-computed","title":"How the Count is Computed","text":"<ol> <li>Per-parameter combination count</li> <li>Independent: <code>pub count \u00d7 sub count</code></li> <li> <p>Constrained: <code>pub count</code> (pub=sub) or <code>sub count</code> (pub-only/sub-only)</p> </li> <li> <p>Pairwise algorithm</p> </li> <li>Build minimal set covering every parameter pair at least once.</li> <li> <p>Actual count depends on parameter interactions.</p> </li> <li> <p>Constraint filtering</p> </li> <li>Remove cases that violate READER_DATA_LIFECYCLE constraints.</li> </ol>"},{"location":"XML_Generator/#sample-run-pp01-rtt02","title":"Sample Run (PP=0.1, RTT=0.2)","text":"<pre><code>Total: 853 test cases\nXML files: 1,706 (853 cases \u00d7 2 files)\n  - pub_qos_case_00001.xml ~ pub_qos_case_00853.xml\n  - sub_qos_case_00001.xml ~ sub_qos_case_00853.xml\n</code></pre>"},{"location":"XML_Generator/#how-to-run","title":"How to Run","text":""},{"location":"XML_Generator/#requirements","title":"Requirements","text":"<ul> <li>Python 3.6 or higher</li> <li>Required: <pre><code>pip install xml\n</code></pre></li> <li>Optional (optimized Pairwise):   <pre><code>pip install allpairspy\n</code></pre> <p>The tool works without <code>allpairspy</code> using the built-in algorithm.</p> </li> </ul>"},{"location":"XML_Generator/#project-structure","title":"Project Structure","text":"<pre><code>XML Generator/\n\u251c\u2500\u2500 xml_generator.py      # Main script\n\u251c\u2500\u2500 xml/\n\u2502   \u251c\u2500\u2500 pub_qos.xml       # Publisher template\n\u2502   \u2514\u2500\u2500 sub_qos.xml       # Subscriber template\n\u2514\u2500\u2500 output/               # Generated XML files\n</code></pre>"},{"location":"XML_Generator/#running-the-generator","title":"Running the Generator","text":""},{"location":"XML_Generator/#option-1-interactive","title":"Option 1: Interactive","text":"<pre><code>cd \"XML Generator\"\npython3 xml_generator.py\n</code></pre> <p>You will be prompted for:</p> <ul> <li>PP (Publication Period): in seconds  </li> <li>RTT (Round Trip Time): in seconds  </li> </ul>"},{"location":"XML_Generator/#option-2-from-code","title":"Option 2: From code","text":"<pre><code>from xml_generator import XMLGenerator\n\ngenerator = XMLGenerator(pp=0.1, rtt=0.2)\ngenerator.generate_all_test_cases(output_dir='output')\n</code></pre>"},{"location":"XML_Generator/#option-3-non-interactive-script","title":"Option 3: Non-interactive (script)","text":"<pre><code>cd \"XML Generator\"\npython3 xml_generator.py &lt;&lt;&lt; $'0.1\\n0.2\\n'\n</code></pre>"},{"location":"XML_Generator/#sample-output","title":"Sample Output","text":"<pre><code>============================================================\nDDS QoS XML Generator\n============================================================\nEnter PP (Publication Period) in seconds: 0.1\nEnter RTT (Round Trip Time) in seconds: 0.2\n\nInput values:\n  PP: 0.1 s\n  RTT: 0.2 s\n  RTT/PP: 2.00\nGenerating test case combinations...\nTotal: 853 test cases\n\nGenerating XML files...\nProgress: 100/853 (11%)\nProgress: 200/853 (23%)\n...\nProgress: 800/853 (93%)\n\nDone! 853 test cases generated.\nOutput directory: output/\n</code></pre>"},{"location":"XML_Generator/#output-files","title":"Output Files","text":"<p>Generated XML files are written to <code>output/</code>:</p> <ul> <li>Publisher: <code>pub_qos_case_00001.xml</code> ~ <code>pub_qos_case_00853.xml</code></li> <li>Subscriber: <code>sub_qos_case_00001.xml</code> ~ <code>sub_qos_case_00853.xml</code></li> </ul> <p>Each file contains the DDS QoS profile with parameter values for that test case.</p>"},{"location":"XML_Generator/#notes","title":"Notes","text":"<ul> <li>PP: Must be greater than 0.</li> <li>Overwrite: Existing files with the same name are overwritten.</li> <li>Directory: <code>output/</code> is created automatically if it does not exist.</li> </ul>"},{"location":"XML_Generator/#reference","title":"Reference","text":""},{"location":"XML_Generator/#dynamic-value-calculation","title":"Dynamic Value Calculation","text":"<p>Some parameter values depend on PP and RTT:</p> <ul> <li>DEADLINE: <code>PP</code>, <code>2\u00d7PP</code></li> <li>LIVELINESS: <code>0.5\u00d7PP</code>, <code>PP</code>, <code>2\u00d7PP</code></li> <li>HISTORY: based on <code>(RTT/PP)+2</code></li> <li>RESOURCE_LIMITS: based on <code>(RTT/PP)+2</code></li> <li>LIFESPAN: <code>0.5\u00d7RTT</code></li> </ul> <p>Case count and values may change with different PP and RTT.</p>"},{"location":"XML_Generator/#constraints-summary","title":"Constraints Summary","text":"Constraint Description ENTITY_FACTORY, DATA_EXISTS pub and sub must be equal READER_DATA_LIFECYCLE NO_WRITER and DISPOSED must be equal WRITER_DATA_LIFECYCLE Publisher only (sub is None) READER_DATA_LIFECYCLE_* Subscriber only (pub is None)"},{"location":"XML_Generator/#license","title":"License","text":"<p>This project is a tool for generating DDS QoS test cases.</p>"},{"location":"rules/","title":"QoS Rules Overview","text":"<p>This section covers 40 dependency-violation rules classified into three stages. </p> <p>Choose a category from the sidebar to see detailed constraints.</p>"},{"location":"rules/#full-list-of-40-dependency-violation-rules","title":"Full List of 40 Dependency-Violation RulesStage 1 | Intra-entity Dependency ValidationStage 2 | Inter-entity Dependency ValidationStage 3 | Timing-based Dependency Validation","text":"<p>We have identified and classified 40 rules that govern the relationships between ROS 2 QoS policies. </p> <p>These are implemented in QoS Guard for static verification. </p> <p>Identifies internal conflicts by analyzing each entity's QoS profile independently.</p> No Identifier QoS Conflict Condition (Violation) Dependency Entity Basis 1 HIST \u2194 RESLIM \\([HIST.kind = KEEP\\_LAST] \\wedge [HIST.depth &gt; RESLIM.mpi]\\) Structural Pub, Sub STD 2 RESLIM \u2194 RESLIM \\([max\\_samples &lt; max\\_samples\\_per\\_instance]\\) Structural Pub, Sub STD 3 RELIAB \u2192 DURABL \\([DURABL \\ge TRAN\\_LOCAL] \\wedge [RELIAB = BEST\\_EFFORT]\\) Functional Pub, Sub IMP 4 RELIAB \u2192 OWNST \\([OWNST = EXCLUSIVE] \\wedge [RELIAB = BEST\\_EFFORT]\\) Functional Pub, Sub IMP 5 RELIAB \u2192 LIVENS \\([LIVENS = MANUAL] \\wedge [RELIAB = BEST\\_EFFORT]\\) Functional Pub, Sub IMP 6 LFSPAN \u2192 DURABL \\([DURABL \\ge TRAN\\_LOCAL] \\wedge [LFSPAN.duration &gt; 0]\\) Functional Pub EMP 7 LFSPAN \u2192 DEADLN \\(LFSPAN.duration &lt; DEADLN.period\\) Functional Sub IMP 8 HIST \u2192 DESTORD \\([DESTORD = BY\\_SOURCE] \\wedge [HIST.kind = KEEP\\_LAST] \\wedge [HIST.depth = 1]\\) Functional Sub IMP 9 RESLIM \u2192 DESTORD \\([DESTORD = BY\\_SOURCE] \\wedge [HIST.kind = KEEP\\_ALL] \\wedge [RESLIM.mpi = 1]\\) Functional Sub IMP 10 DEADLN \u2192 OWNST \\([OWNST = EXCLUSIVE] \\wedge [DEADLN.period = \\infty]\\) Functional Sub IMP 11 LIVENS \u2192 OWNST \\([OWNST = EXCLUSIVE] \\wedge [LIVENS.lease = \\infty]\\) Functional Sub IMP 12 LIVENS \u2192 RDLIFE \\([autopurge\\_nowriter &gt; 0] \\wedge [LIVENS.lease = \\infty]\\) Functional Sub IMP 13 RDLIFE \u2192 DURABL \\([DURABL \\ge TRANSIENT] \\wedge [autopurge\\_disposed \\neq \\infty]\\) Functional Sub IMP 14 PART \u2192 DEADLN \\([DEADLN.period &gt; 0] \\wedge [PART.names \\neq \\emptyset]\\) Functional Sub IMP 15 PART \u2192 LIVENS \\([LIVENS = MANUAL] \\wedge [PART.names \\neq \\emptyset]\\) Functional Sub IMP 16 OWNST \u2192 WDLIFE \\([autodispose = TRUE] \\wedge [OWNST = EXCLUSIVE]\\) Functional Sub IMP 17 HIST \u2192 LFSPAN \\([HIST.kind=KEEP\\_LAST] \\wedge [LFSPAN.duration &gt; HIST.depth \\times PP]\\) Operational Pub, Sub IMP 18 RESLIM \u2192 LFSPAN \\([HIST.kind=KEEP\\_ALL] \\wedge [LFSPAN.duration &gt; RESLIM.mpi \\times PP]\\) Operational Pub, Sub IMP 19 ENTFAC \u2192 DURABL \\([DURABL = VOLATILE] \\wedge [autoenable = FALSE]\\) Operational Pub, Sub IMP 20 PART \u2192 DURABL \\([DURABL \\ge TRAN\\_LOCAL] \\wedge [PART.names \\neq \\emptyset]\\) Operational Pub, Sub IMP <p>Prevents connection failures by checking RxO compatibility between Publisher and Subscriber pairs.</p> No Identifier QoS Conflict Condition (Violation) Dependency Entity Basis 21 PART \u2194 PART \\([Writer.PART \\cap Reader.PART] = \\emptyset\\) Structural Pub \u2194 Sub STD 22 RELIAB \u2194 RELIAB \\([Writer.RELIAB &lt; Reader.RELIAB]\\) Structural Pub \u2194 Sub STD 23 DURABL \u2194 DURABL \\([Writer.DURABL &lt; Reader.DURABL]\\) Structural Pub \u2194 Sub STD 24 DEADLN \u2194 DEADLN \\([Writer.DEADLN.period &gt; Reader.DEADLN.period]\\) Structural Pub \u2194 Sub STD 25 LIVENS \u2194 LIVENS \\([W.LIVENS &lt; R.LIVENS] \\vee [W.lease &gt; R.lease]\\) Structural Pub \u2194 Sub STD 26 OWNST \u2194 OWNST \\([Writer.OWNST \\neq Reader.OWNST]\\) Structural Pub \u2194 Sub STD 27 DESTORD \u2194 DESTORD \\([Writer.DESTORD &lt; Reader.DESTORD]\\) Structural Pub \u2194 Sub STD 28 WDLIFE \u2192 RDLIFE \\([W.autodispose = FALSE] \\wedge [R.autopurge\\_nowriter = 0]\\) Functional Pub \u2194 Sub IMP 29 WDLIFE \u2192 RDLIFE \\([W.autodispose = FALSE] \\wedge [R.autopurge\\_disposed &gt; 0]\\) Operational Pub \u2194 Sub IMP 30 WDLIFE \u2192 RDLIFE \\([W.autodispose = FALSE] \\wedge [R.autopurge\\_nowriter = \\infty]\\) Operational Pub \u2194 Sub IMP <p>Evaluates operational risks by integrating network parameters like RTT and publish period.</p> No Identifier QoS Conflict Condition (Violation) Dependency Entity Basis 31 HIST \u2192 RELIAB \\([RELIABLE] \\wedge [HIST.kind=KEEP\\_LAST] \\wedge [HIST.depth &lt; \\lceil 2 \\times RTT/PP \\rceil + 1]\\) Functional Pub EMP 32 RESLIM \u2192 RELIAB \\([RELIABLE] \\wedge [HIST.kind=KEEP\\_ALL] \\wedge [RESLIM.mpi &lt; \\lceil 2 \\times RTT/PP \\rceil + 1]\\) Functional Pub EMP 33 LFSPAN \u2192 RELIAB \\([RELIABLE] \\wedge [LFSPAN.duration &lt; PP + 2 \\times RTT]\\) Functional Pub EMP 34 RELIAB \u2192 WDLIFE \\([autodispose = TRUE] \\wedge [RELIAB = BEST\\_EFFORT]\\) Functional Pub IMP 35 RELIAB \u2192 DEADLN \\([DEADLN.period &gt; 0] \\wedge [RELIAB = BEST\\_EFFORT]\\) Functional Pub \u2194 Sub IMP 36 LIVENS \u2192 DEADLN \\([DEADLN.period &gt; 0] \\wedge [LIVENS.lease &lt; DEADLN.period]\\) Functional Sub EMP 37 HIST \u2192 DURABL \\([DURABL \\ge TRAN\\_LOCAL] \\wedge [HIST.kind=KEEP\\_ALL] \\wedge [RESLIM.mpi \\ge default]\\) Operational Pub EMP 38 DEADLN \u2192 OWNST \\([OWNST = EXCLUSIVE] \\wedge [DEADLN.period &lt; PP +  2 \\times RTT]\\) Operational Sub EMP 39 LIVENS \u2192 OWNST \\([OWNST = EXCLUSIVE] \\wedge [LIVENS.lease &lt; PP +  2 \\times RTT]\\) Operational Sub EMP 40 DURABL \u2192 DEADLN \\([DEADLN.period &gt; 0] \\wedge [DURABL \\ge TRAN\\_LOCAL]\\) Operational Sub EMP <p>Notation Summary</p> <ul> <li>mpi: <code>max_samples_per_instance</code> </li> <li>PP: <code>Publish Period</code> </li> <li>RTT: <code>Round Trip Time</code></li> </ul>"},{"location":"rules/EMP/","title":"EMP Rules","text":"<p>This page describes the QoS dependency rules derived from Empirical analysis and experimental results. These rules focus on runtime performance, network conditions (e.g., RTT), and timing-critical dependencies that were validated through systematic testing.</p>"},{"location":"rules/EMP/#stage-1","title":"Stage 1","text":"<p>Intra-entity Dependency Validation</p> 6 LFSPAN \u2192 DURABL Functional        [DURABL \u2265 TRAN_LOCAL] \u2227 [LFSPAN.duration &gt; 0]      Entity Pub Basis EMP"},{"location":"rules/EMP/#stage-3","title":"Stage 3","text":"<p>Timing-based Dependency Validation</p> 31 HIST \u2192 RELIAB Functional        [RELIABLE] \u2227 [HIST.kind = KEEP_LAST] \u2227 [HIST.depth &lt; \u2308 2 \u00d7 RTT/PP \u2309 + 1]      Entity Pub Basis EMP 32 RESLIM \u2192 RELIAB Functional        [RELIABLE] \u2227 [HIST.kind = KEEP_ALL] \u2227 [RESLIM.mpi &lt; \u2308 2 \u00d7 RTT/PP \u2309 + 1]      Entity Pub Basis EMP 33 LFSPAN \u2192 RELIAB Functional        [RELIABLE] \u2227 [LFSPAN.duration &lt; PP + 2 \u00d7 RTT]      Entity Pub Basis EMP 35 RELIAB \u2192 DEADLN Functional        [DEADLN.period &gt; 0] \u2227 [RELIAB = BEST_EFFORT]      Entity Sub Basis EMP 36 LIVENS \u2192 DEADLN Functional        [DEADLN.period &gt; 0] \u2227 [LIVENS.lease &lt; DEADLN.period]      Entity Sub Basis EMP 37 HIST \u2192 DURABL Operational        [DURABL \u2265 TRAN_LOCAL] \u2227 [HIST.kind = KEEP_ALL] \u2227 [RESLIM.mpi \u2265 default]      Entity Pub Basis EMP 38 DEADLN \u2192 OWNST Operational        [OWNST = EXCLUSIVE] \u2227 [DEADLN.period &lt; PP + 2 \u00d7 RTT]      Entity Sub Basis EMP 39 LIVENS \u2192 OWNST Operational        [OWNST = EXCLUSIVE] \u2227 [LIVENS.lease &lt; PP + 2 \u00d7 RTT]      Entity Sub Basis EMP <p>Notation Summary</p> <ul> <li>mpi: <code>max_samples_per_instance</code> </li> <li>PP: <code>Publish Period</code> </li> <li>RTT: <code>Round Trip Time</code></li> </ul>"},{"location":"rules/EMP/#experimental-evidence-details","title":"Experimental Evidence Details","text":""},{"location":"rules/EMP/#rule-6","title":"Rule 6","text":"<p>Validates why Durability (Transient Local) requires a non-zero Lifespan to provide late-joining data.</p> <p>1. Experimental Setup</p> <ul> <li>Publisher: Durability = <code>TRANSIENT_LOCAL</code>, Lifespan = <code>50ms</code></li> <li>Subscriber 1 (Existing): Launched before Publisher.</li> <li>Subscriber 2 (Late-joiner): Launched after Publisher finishes sending 1,000 samples.</li> <li>Total Samples Sent: 1,000</li> </ul> <p>2. Test Scenario</p> <ol> <li>Launch Subscriber 1 to monitor live data.</li> <li>Launch Publisher and transmit 1,000 samples (Total time taken &gt; 50ms).</li> <li>Confirm Subscriber 1 received all 1,000 samples.</li> <li>Launch Subscriber 2 (Late-joiner) to retrieve historical data from the Publisher's buffer.</li> </ol> <p>3. Experimental Observation</p> Entity Expected Received Actual Received Status Subscriber 1 (Live) 1,000 1,000 \u2705 Success Subscriber 2 (Late) 1,000 0 \u274c Data Expired <p>4. Empirical Conclusion</p> <p>Even though <code>TRANSIENT_LOCAL</code> is set to store data for late-joiners, the Lifespan (50ms) caused all buffered samples to be purged from the Publisher's queue before Subscriber 2 could connect.</p> <p></p>"},{"location":"rules/EMP/#rule-31","title":"Rule 31","text":"<p>Justifies the minimum History Depth required to prevent data loss in Reliable communication under network delay and loss.</p> <p>1. Experimental Setup</p> <ul> <li>QoS Profile: Reliability = <code>RELIABLE</code>, History Kind = <code>KEEP_LAST</code></li> <li>Network Condition (Loss): 5% Packet Loss (Simulated via <code>tc</code>)</li> <li>Network Condition (Delay): 100ms to 500ms RTT (Round Trip Time)</li> <li>Publication Period (PP): 100ms (10Hz)</li> <li>Variable: History Depth (\\(1 \\sim N\\))</li> </ul> <p>2. Test Scenario</p> <ol> <li>Set the network packet loss to 5% and RTT to a range of 100ms to 500ms.</li> <li>Transmit 1,000 samples from Publisher to Subscriber.</li> <li>Decrease the History Depth incrementally for each test run.</li> </ol> <p>3. Experimental Observation</p> <p>4. Empirical Conclusion</p> <p>In a lossy network (5% loss), a Reliable connection requires retransmission of lost packets. If the History Depth is smaller than the number of samples sent during one RTT, the buffer is overwritten before a retransmission can be requested. </p> <p></p>"},{"location":"rules/EMP/#rule-32","title":"Rule 32","text":"<p>Justifies the minimum Resource Limits (max_samples_per_instance) required to sustain reliable transmission under network delay.</p> <p>1. Experimental Setup</p> <ul> <li>QoS Profile: Reliability = <code>RELIABLE</code>, History Kind = <code>KEEP_ALL</code></li> <li>Resource Limits: <code>max_samples_per_instance</code> (Variable: \\(1 \\sim N\\))</li> <li>Network Condition: packet loss 5%</li> <li>Network Latency (RTT): 100ms, 200ms, 300ms, 400ms (Simulated via <code>tc</code>)</li> <li>Publication Period (PP): 100ms (10Hz)</li> </ul> <p>2. Test Scenario</p> <ol> <li>Connect two notebooks and verify the baseline RTT.</li> <li>Set the Publisher's History to <code>KEEP_ALL</code> to ensure all samples are subject to Resource Limits.</li> <li>Vary the RTT from 100ms to 400ms using network emulation tools.</li> <li>Decrease <code>max_samples_per_instance</code> until sample rejected or lost events occur.</li> <li>Record the minimum <code>mpi</code> value that ensures 100% successful delivery.</li> </ol> <p>3. Experimental Observation</p> <p>4. Empirical Conclusion</p> <p>When using <code>KEEP_ALL</code>, the <code>max_samples_per_instance</code> (mpi) acts as the effective buffer size for the reliability protocol. If mpi is insufficient to hold all samples sent during one RTT (plus the time for ACK/NACK processing), the Publisher will either block or drop samples.</p> <p></p>"},{"location":"rules/EMP/#rule-33","title":"Rule 33","text":"<p>Justifies the minimum Lifespan duration required to ensure sample reception before expiration in reliable communication.</p> <p>1. Experimental Setup</p> <ul> <li>QoS Profile: Reliability = <code>RELIABLE</code>, Lifespan = <code>Variable</code></li> <li>Publication Period (PP): 10ms to 100ms</li> <li>Lifespan Duration: 100ms to 1000ms</li> <li>Total Samples: 10,000</li> </ul> <p>2. Test Scenario</p> <ol> <li>Fix the total number of samples to be sent at 10,000.</li> <li>Perform a grid search by varying the Publish Period (PP) from 10ms to 100ms and Lifespan duration from 100ms to 1000ms.</li> <li>Measure the total number of samples successfully received at the Subscriber.</li> <li>Identify the threshold where sample reception starts to drop despite using <code>RELIABLE</code> QoS.</li> </ol> <p>3. Experimental Observation</p> <ul> <li>High Reception Zone (Blue): When Lifespan is sufficiently longer than RTT and PP, nearly all 10,000 samples are received.</li> <li>Low Reception Zone (White/Gray): When Lifespan duration is close to or shorter than the round-trip retransmission time, samples expire before they can be successfully delivered or processed.</li> </ul> <p>4. Empirical Conclusion</p> <p>Even with <code>RELIABLE</code> settings, data loss occurs if the Lifespan duration is shorter than the time required for successful transmission and acknowledgement. </p> <p></p>"},{"location":"rules/EMP/#rule-35","title":"Rule 35","text":"<p>Justifies how Reliability settings impact Deadline compliance under packet loss conditions.</p> <p>1. Experimental Setup</p> <ul> <li>QoS Profile: Deadline Period = <code>150ms</code>, Reliability = <code>RELIABLE</code> vs <code>BEST_EFFORT</code></li> <li>Network Condition (Loss): 5% Packet Loss (Simulated via <code>tc</code>)</li> <li>Publication Period (PP): 100ms</li> <li>Total Samples: 1,000</li> </ul> <p>2. Test Scenario</p> <ol> <li>Set the Publisher and Subscriber with a Deadline period of 150ms.</li> <li>Introduce 5% packet loss to the network interface.</li> <li>Case A: Configure both entities with <code>RELIABLE</code> Reliability.</li> <li>Case B: Configure both entities with <code>BEST_EFFORT</code> Reliability.</li> <li>Publish 1,000 samples and monitor the <code>on_requested_deadline_missed</code> callback at the Subscriber.</li> </ol> <p>3. Experimental Observation</p> Reliability Total Samples Received Samples Deadline Missed Count Cause of Violation RELIABLE 1,000 1,000 25 Delay caused by Retransmission BEST_EFFORT 1,000 ~950 48 Gap caused by Packet Loss <p>4. Empirical Conclusion</p> <p>The experimental results highlight two different causes of Deadline violations: 1. In RELIABLE mode, all 1,000 samples were received, but 25 samples violated the Deadline because the time taken for NACK-based retransmission exceeded the 150ms window. 2. In BEST_EFFORT mode, violations nearly doubled (48 times) because lost packets created permanent gaps in the data stream, directly triggering the Deadline timer.</p> <p></p>"},{"location":"rules/EMP/#rule-36","title":"Rule 36","text":"<p>Justifies the consistency requirement between Liveliness Lease Duration and Deadline Period to avoid redundant or conflicting fault detections.</p> <p>1. Experimental Setup</p> <ul> <li>Publication Period (PP): 100ms</li> <li>Deadline Period: 200ms</li> <li>Total Samples: 600 (Total duration: 60s)</li> <li>Liveliness Lease Duration: Variable</li> </ul> <p>2. Test Scenario</p> <p>The network condition is controlled over 60 seconds using the <code>tc</code> command:</p> <ol> <li>0s ~ 20s (Loss 0%): Normal communication.</li> <li>20s ~ 40s (Loss 10%): Induce occasional Deadline Missed due to retransmission delays.</li> <li>40s ~ 45s (Loss 100%): Induce Liveliness Lost by blocking all packets.</li> <li>45s ~ 60s (Loss 0%): Restore communication and observe recovery.</li> </ol> <p>3. Experimental Observation &amp; Empirical Conclusion</p> <p>[ISSUE] Liveliness Lost is triggered prematurely (approx. 60~80ms). Even though the Publisher is already marked as 'Offline', Deadline Missed alarms continue to trigger late at the 200ms mark.</p> <p>[Conclusion] The experiment demonstrates that when <code>Lease Duration &lt; Deadline</code>, the system falls into a contradictory state: it continues to fire \"Data Missing\" alarms (Deadline Missed) for a Publisher that it has already declared \"Dead\" (Liveliness Lost). </p> <p>To prevent this state inconsistency and ensure a logical fault-detection sequence (where the data stream is monitored within the lifespan of the entity), the Liveliness Lease Duration must always be longer than the Deadline Period: \\(LIVENS.lease \u2265 DEADLN.period\\)</p> <p></p>"},{"location":"rules/EMP/#rule-37","title":"Rule 37","text":"<p>Justifies the impact of Resource Limits on the recovery latency for late-joining subscribers in durable communication.</p> <p>1. Experimental Setup</p> <ul> <li>QoS Profile: Durability = <code>TRANSIENT_LOCAL</code>, History = <code>KEEP_ALL</code></li> <li>Variable: <code>max_samples_per_instance</code> (mpi) ranging from 1,000 to 5,000</li> <li>Data Volume: Sample count up to 1,600</li> <li>Metric: Latency (ms) required for the system to reach steady-state convergence</li> </ul> <p>2. Test Scenario</p> <ol> <li>Initialize a Publisher that sends samples continuously with <code>TRANSIENT_LOCAL</code> durability.</li> <li>After a specific number of samples are published, introduce a Late-Joining Subscriber.</li> <li>Vary the max_samples_per_instance setting on the Subscriber side.</li> <li>Measure the latency between the sample generation time and the time it is actually processed by the late-joiner.</li> <li>Observe how long the \"catch-up\" phase lasts before latency returns to normal levels.</li> </ol> <p>3. Experimental Observation</p> <ul> <li>Low mpi Zone: Recovery is relatively fast</li> <li>High mpi Zone: As <code>max_samples_per_instance</code> increases towards 5,000, the recovery latency increases exponentially (reaching up to 50,000ms). The system takes significantly longer to converge to a steady state.</li> </ul> <p>4. Empirical Conclusion</p> <p>The experiment proves that setting an excessively high <code>max_samples_per_instance</code> for the sake of data completeness leads to a \"Recovery Latency Storm.\" For a late-joining node, processing thousands of buffered historical samples simultaneously saturates the available bandwidth and CPU, causing current data to be delayed by tens of seconds.</p> <p>To balance data durability with system responsiveness, the resource limits must be tuned based on the expected maximum network downtime and the acceptable recovery window: \\([DURABL \u2265 TRAN_LOCAL] \u2227 [KEEP_ALL] \u21d2 mpi \u2265 default\\) (Note: \\(default\\) should be calculated based on the maximum tolerable recovery time.)</p> <p></p>"},{"location":"rules/EMP/#rule-38","title":"Rule 38","text":"<p>Justifies the minimum Deadline period required to maintain stable Ownership in exclusive communication under network instability.</p> <p>1. Experimental Setup</p> <ul> <li>QoS Profile: Ownership = <code>EXCLUSIVE</code>, Reliability = <code>RELIABLE</code></li> <li>Variable: Deadline Period (<code>100ms</code> vs <code>500ms</code>)</li> <li>Network Condition: 5% Packet Loss (Simulated via <code>tc</code>)</li> <li>Publication Period (PP): 100ms</li> <li>Total Samples: 400</li> </ul> <p>2. Test Scenario</p> <ol> <li>Configure two Publishers with different strengths for <code>EXCLUSIVE</code> ownership.</li> <li>Set both Publishers and the Subscriber to <code>RELIABLE</code> reliability to ensure eventual delivery.</li> <li>Introduce a constant 5% packet loss using the <code>tc</code> command.</li> <li>Case A: Set the Deadline period to <code>100ms</code> (equal to the PP).</li> <li>Case B: Set the Deadline period to <code>500ms</code> (\\(5 \u00d7 PP\\)).</li> <li>Monitor the <code>on_requested_deadline_missed</code> callback and track how often the active owner is switched due to deadline timeouts.</li> </ol> <p>3. Experimental Observation</p> <ul> <li>Case A (Deadline = 100ms - Orange): Numerous spikes are observed throughout the experiment. Any minor retransmission delay caused by the 5% loss immediately triggers a deadline miss, leading to unstable ownership.</li> <li>Case B (Deadline = 500ms - Blue): Significantly fewer and more sparse spikes appear. The larger window provides sufficient time for the <code>RELIABLE</code> protocol to recover lost packets through NACK/retransmission before the 500ms timer expires, maintaining a more consistent owner state.</li> </ul> <p>4. Empirical Conclusion</p> <p>The experiment clearly demonstrates that Ownership Stability is highly dependent on the Deadline period in lossy networks. A tight deadline (Case A) causes \"Ownership Churning,\" where control is frequently and unnecessarily handed over due to transient network jitters. </p> <p>By extending the Deadline (Case B), the system becomes more resilient to packet loss, ensuring that the primary owner remains active as long as retransmissions are successful within the extended margin. To ensure stable control in exclusive ownership, the Deadline should be at least twice the Publication Period: \\([OWNST = EXCLUSIVE] \u21d2 DEADLN.period \u2265 2 \u00d7 PP\\)</p> <p></p>"},{"location":"rules/EMP/#rule-39","title":"Rule 39","text":"<p>Justifies the impact of Liveliness Lease Duration on Ownership stability by preventing premature entity failure detection.</p> <p>1. Experimental Setup</p> <ul> <li>QoS Profile: Ownership = <code>EXCLUSIVE</code>, Liveliness = <code>AUTOMATIC</code></li> <li>Variable (Lease Duration): 50ms, 100ms, 300ms, 500ms, 800ms</li> <li>Network Condition: 80% Packet Loss (Simulated via <code>tc</code>)</li> <li>Publication Period (PP): 100ms</li> <li>Metric: Total count of Liveliness Lost events</li> </ul> <p>2. Test Scenario</p> <ol> <li>Initialize two Publishers with different strengths for <code>EXCLUSIVE</code> ownership.</li> <li>Apply a harsh network environment with 80% packet loss using the <code>tc</code> command.</li> <li>Publish a total of 10,000 samples to observe long-term stability.</li> <li>Vary the Liveliness Lease Duration from 50ms to 800ms.</li> <li>Record the frequency of ownership handovers triggered by <code>on_liveliness_changed</code> at the Subscriber side.</li> </ol> <p>3. Experimental Observation</p> <ul> <li>Short Lease Duration (50ms - 100ms): A high frequency of Liveliness Lost events is observed (up to 26 times). Since the lease duration is shorter than or equal to the Publication Period (100ms), even a single delayed heartbeat triggers a failure detection.</li> <li>Long Lease Duration (300ms - 800ms): The event count drops significantly as the lease duration increases. At 800ms, only 10 events occur, indicating that the system can tolerate multiple lost or delayed heartbeats without dropping the ownership.</li> </ul> <p>4. Empirical Conclusion</p> <p>The experiment highlights the risk of \"False Positive Failures.\" When the Liveliness Lease Duration is too aggressive (close to the \\(PP\\)), transient network issues cause the Subscriber to frequently revoke ownership from the primary Publisher. This leads to unstable system control and unnecessary handovers.</p> <p>To maintain a stable Ownership state, the Liveliness Lease Duration must be set with enough margin to accommodate at least two consecutive heartbeat losses or retransmission delays: \\([OWNST = EXCLUSIVE] \u21d2 lease_duration \u2265 2 \u00d7 PP\\)</p>"},{"location":"rules/IMP/","title":"IMP Rules","text":"<p># IMP Rules</p> <p>This page describes the QoS dependency rules derived from the specific implementation behaviors of ROS 2 Middlewares (RMWs) such as eProsima Fast DDS and Eclipse Cyclone DDS. These dependencies are not explicitly mandated by the DDS standard but are critical for functional consistency in practice.</p>"},{"location":"rules/IMP/#stage-1","title":"Stage 1","text":"<p>Intra-entity Dependency Validation</p> 3 RELIAB \u2192 DURABL Functional        [DURABL \u2265 TRAN_LOCAL] \u2227 [RELIAB = BEST_EFFORT]      Entity Pub, Sub Basis IMP 4 RELIAB \u2192 OWNST Functional        [OWNST = EXCLUSIVE] \u2227 [RELIAB = BEST_EFFORT]      Entity Pub, Sub Basis IMP 5 RELIAB \u2192 LIVENS Functional        [LIVENS = MANUAL] \u2227 [RELIAB = BEST_EFFORT]      Entity Pub, Sub Basis IMP 7 LFSPAN \u2192 DEADLN Functional        LFSPAN.duration &lt; DEADLN.period      Entity Sub Basis IMP 8 HIST \u2192 DESTORD Functional        [DESTORD = BY_SOURCE] \u2227 [HIST.kind = KEEP_LAST] \u2227 [HIST.depth = 1]      Entity Sub Basis IMP 9 RESLIM \u2192 DESTORD Functional        [DESTORD = BY_SOURCE] \u2227 [HIST.kind = KEEP_ALL] \u2227 [RESLIM.mpi = 1]      Entity Sub Basis IMP 10 DEADLN \u2192 OWNST Functional        [OWNST = EXCLUSIVE] \u2227 [DEADLN.period = \u221e]      Entity Sub Basis IMP 11 LIVENS \u2192 OWNST Functional        [OWNST = EXCLUSIVE] \u2227 [LIVENS.lease = \u221e]      Entity Sub Basis IMP 12 LIVENS \u2192 RDLIFE Functional        [autopurge_nowriter &gt; 0] \u2227 [LIVENS.lease = \u221e]      Entity Sub Basis IMP 13 RDLIFE \u2192 DURABL Functional        [DURABL \u2265 TRANSIENT] \u2227 [autopurge_disposed \u2260 \u221e]      Entity Sub Basis IMP 14 PART \u2192 DEADLN Functional        [DEADLN.period &gt; 0] \u2227 [PART.names \u2260 \u2205]      Entity Sub Basis IMP 15 PART \u2192 LIVENS Functional        [LIVENS = MANUAL] \u2227 [PART.names \u2260 \u2205]      Entity Sub Basis IMP 16 OWNST \u2192 WDLIFE Functional        [autodispose = TRUE] \u2227 [OWNST = EXCLUSIVE]      Entity Sub Basis IMP 17 HIST \u2192 LFSPAN Operational        [HIST.kind = KEEP_LAST] \u2227 [LFSPAN.duration &gt; HIST.depth \u00d7 PP]      Entity Pub, Sub Basis IMP 18 RESLIM \u2192 LFSPAN Operational        [HIST.kind = KEEP_ALL] \u2227 [LFSPAN.duration &gt; RESLIM.mpi \u00d7 PP]      Entity Pub, Sub Basis IMP 19 ENTFAC \u2192 DURABL Operational        [DURABL = VOLATILE] \u2227 [autoenable = FALSE]      Entity Pub, Sub Basis IMP 20 PART \u2192 DURABL Operational        [DURABL \u2265 TRAN_LOCAL] \u2227 [PART.names \u2260 \u2205]      Entity Pub, Sub Basis IMP"},{"location":"rules/IMP/#stage-2","title":"Stage 2","text":"<p>Inter-entity Dependency Validation</p> 28 WDLIFE \u2192 RDLIFE Functional        [W.autodispose = FALSE] \u2227 [R.autopurge_nowriter = 0]      Entity Pub \u2194 Sub Basis IMP 29 WDLIFE \u2192 RDLIFE Operational        [W.autodispose = FALSE] \u2227 [R.autopurge_disposed &gt; 0]      Entity Pub \u2194 Sub Basis IMP 30 WDLIFE \u2192 RDLIFE Operational        [W.autodispose = FALSE] \u2227 [R.autopurge_nowriter = \u221e]      Entity Pub \u2194 Sub Basis IMP"},{"location":"rules/IMP/#stage-3","title":"Stage 3","text":"<p>Dynamic &amp; Performance Rules</p> 34 RELIAB \u2192 WDLIFE Functional        [autodispose = TRUE] \u2227 [RELIAB = BEST_EFFORT]      Entity Pub Basis IMP 40 DURABL \u2192 DEADLN Operational        [DEADLN.period &gt; 0] \u2227 [DURABL \u2265 TRAN_LOCAL]      Entity Sub Basis IMP"},{"location":"rules/IMP/#implementation-evidence-details","title":"Implementation Evidence Details","text":"<p>Below are the code-level justifications and source references for each IMP rule.</p> <p></p>"},{"location":"rules/IMP/#rule-3","title":"Rule 3","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// [TRANSIENT_LOCAL late-joiner logic resides within if (is_reliable)]\n// \u2192 When best-effort, it returns false, preventing the later-joiner logic from executing \u2192 Transient behaviour does not occur\nbool is_reliable = rp-&gt;is_reliable(); \nif (is_reliable) \n{ SequenceNumber_t min_seq = get_seq_num_min(); \nSequenceNumber_t last_seq = get_seq_num_max(); \nRTPSMessageGroup group(mp_RTPSParticipant, this, rp-&gt;message_sender()); \n// History not empty \nif (min_seq != SequenceNumber_t::unknown()) \n{ (void)last_seq; assert(last_seq != SequenceNumber_t::unknown()); assert(min_seq &lt;= last_seq);\n try { \n// Late-joiner \nif (TRANSIENT_LOCAL &lt;= rp-&gt;durability_kind() &amp;&amp; TRANSIENT_LOCAL &lt;= m_att.durabilityKind)\n</code></pre></li> <li>RMW/Implementation: CycloneDDS <pre><code>// Providing historical data to late joiners is only performed when it is more efficient than best-effort processing.\n  /* Store available data into the late joining reader when it is reliable (we don't do\nhistorical data for best-effort data over the wire, so also not locally). */\nif (rd-&gt;xqos-&gt;reliability.kind &gt; DDS_RELIABILITY_BEST_EFFORT &amp;&amp; rd-&gt;xqos-&gt;durability.kind &gt; DDS_DURABILITY_VOLATILE)\nddsi_deliver_historical_data (wr, rd);\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-4","title":"Rule 4","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// Only the FastDDS code contains code that checks this rule.\nif (m_reliability.kind == BEST_EFFORT_RELIABILITY_QOS &amp;&amp; m_ownership.kind == EXCLUSIVE_OWNERSHIP_QOS) \n{\nlogError(RTPS_QOS_CHECK, \"BEST_EFFORT incompatible with EXCLUSIVE ownership\"); \nreturn false; \n} \nreturn true;\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-5","title":"Rule 5","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// When using best effort, a StatelessWriter is created.\n// When using Manual_by_topic, a heartbeat is sent only if it is a StatefulWriter \u21d2 No heartbeat is sent.\nif (qos_.liveliness().kind == MANUAL_BY_TOPIC_LIVELINESS_QOS) \n{ \n// As described in the RTPS specification, if liveliness kind is manual a heartbeat must be sent // This only applies to stateful writers, as stateless writers do not send heartbeats \nStatefulWriter* stateful_writer = dynamic_cast&lt;StatefulWriter*&gt;(writer_); \nif (stateful_writer != nullptr) \n{ stateful_writer-&gt;send_periodic_heartbeat(true, true); } } \nreturn RETCODE_OK;\n</code></pre></li> <li>RMW/Implementation: CycloneDDS <pre><code>// Only when reliable \u2192 HB transmission\n// When best-effort, loss is possible, so live link status determination may be incorrect\n/* heartbeat event will be deleted when the handler can't find a\nwriter for it in the hash table. NEVER =&gt; won't ever be\nscheduled, and this can only change by writing data, which won't\nhappen until after it becomes visible. */\nif (wr-&gt;reliable)\nwr-&gt;heartbeat_xevent = qxev_heartbeat (wr-&gt;evq, DDSRT_MTIME_NEVER, &amp;wr-&gt;e.guid);\nelse\nwr-&gt;heartbeat_xevent = NULL;\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-7","title":"Rule 7","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// The Reader's Lifespan and Deadline timers share the same History \n!History \ndetail::DataReaderHistory history_;\n</code></pre></li> <li>RMW/Implementation: CycloneDDS <pre><code>// Both timers operate independently\n\n// Deadline\nddsrt_mtime_t deadline_next_missed_locked (struct deadline_adm *deadline_adm, ddsrt_mtime_t tnow, void **instance)\n{\nstruct deadline_elem *elem = NULL;\nif (!ddsrt_circlist_isempty (&amp;deadline_adm-&gt;list))\n{\nstruct ddsrt_circlist_elem *list_elem = ddsrt_circlist_oldest (&amp;deadline_adm-&gt;list);\nelem = DDSRT_FROM_CIRCLIST (struct deadline_elem, e, list_elem);\nif (elem-&gt;t_deadline.v &lt;= tnow.v)\n{\nddsrt_circlist_remove (&amp;deadline_adm-&gt;list, &amp;elem-&gt;e);\nif (instance != NULL)\n*instance = (char *)elem - deadline_adm-&gt;elem_offset;\nreturn (ddsrt_mtime_t) { 0 };\n}\n}\nif (instance != NULL)\n*instance = NULL;\nreturn (elem != NULL) ? elem-&gt;t_deadline : DDSRT_MTIME_NEVER;\n}\n\n// Lifespan\nddsrt_mtime_t lifespan_next_expired_locked (const struct lifespan_adm *lifespan_adm, ddsrt_mtime_t tnow, void **sample)\n{\nstruct lifespan_fhnode *node;\nif ((node = ddsrt_fibheap_min(&amp;lifespan_fhdef, &amp;lifespan_adm-&gt;ls_exp_heap)) != NULL &amp;&amp; node-&gt;t_expire.v &lt;= tnow.v)\n{\n*sample = (char *)node - lifespan_adm-&gt;fhn_offset;\nreturn (ddsrt_mtime_t) { 0 };\n}\n*sample = NULL;\nreturn (node != NULL) ? node-&gt;t_expire : DDSRT_MTIME_NEVER;\n}\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-8","title":"Rule 8","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// Removal is performed by comparing the sourceTimestamp; if depth=1, existing older data is simply deleted.\n// Therefore, as there is only one piece of archived data, there is nothing to compare it with, rendering the process meaningless.\n// Try to substitute the oldest sample.\nCacheChange_t* first_change = instance_changes.at(0);\nif (a_change-&gt;sourceTimestamp &gt;= first_change-&gt;sourceTimestamp)\n{\n// As the instance is ordered by source timestamp, we can always remove the first one.\nret_value = remove_change_sub(first_change);\n}\nelse\n{\n// Received change is older than oldest, and should be discarded\nreturn true;\n}\n</code></pre></li> <li>RMW/Implementation: CycloneDDS <pre><code>// In BY_SOURCE_TIMESTAMP mode, samples that are \"reversed relative to the source timestamp (i.e., past-time samples arriving late)\" are discarded unconditionally.\n// This behaviour results in retaining only the single most recent sample, rather than performing any sorting function.\nstatic int inst_accepts_sample (const struct dds_rhc_default *rhc, const struct rhc_instance *inst, const struct ddsi_writer_info *wrinfo, const struct ddsi_serdata sample, const bool has_data)\n{ if (rhc-&gt;by_source_ordering) {\n  if (sample-&gt;timestamp.v &gt; inst-&gt;tstamp.v)\n{/ ok /}\nelse if (sample-&gt;timestamp.v &lt; inst-&gt;tstamp.v)\n{return 0;}\nelse if (inst_accepts_sample_by_writer_guid (inst, wrinfo))\n{/ ok /}\nelse\n{return 0;}}\nif (rhc-&gt;exclusive_ownership &amp;&amp; inst-&gt;wr_iid_islive &amp;&amp; inst-&gt;wr_iid != wrinfo-&gt;iid)\n{int32_t strength = wrinfo-&gt;ownership_strength;\nif (strength &gt; inst-&gt;strength) {\n/ ok /\n} else if (strength &lt; inst-&gt;strength) {\nreturn 0;\n} else if (inst_accepts_sample_by_writer_guid (inst, wrinfo)) {\n/ ok /\n} else {return 0;}}\nif (has_data &amp;&amp; !content_filter_accepts (rhc-&gt;reader, sample, inst, wrinfo-&gt;iid, inst-&gt;iid))\n{return 0;}\nreturn 1;}\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-9","title":"Rule 9","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// The KeepAll policy is capped at 1 by resourcelimits, meaning no new samples are added once the buffer is full.\n// Consequently, with only one piece of data being retained, there is nothing to compare it with, rendering it meaningless.\n(DataReaderHistory.cpp)\nInstanceCollection::iterator vit;\nif (find_key(a_change-&gt;instanceHandle, vit)) \n{ DataReaderInstance::ChangeCollection&amp; instance_changes = vit-&gt;second-&gt;cache_changes; \nsize_t total_size = instance_changes.size() + unknown_missing_changes_up_to; \nif (total_size &lt; static_cast&lt;size_t&gt;(resource_limited_qos_.max_samples_per_instance)) \n{ \nreturn add_received_change_with_key(a_change, *vit-&gt;second); \n} \nlogInfo(SUBSCRIBER, \"Change not added due to maximum number of samples per instance\"); }\n</code></pre></li> <li>RMW/Implementation: CycloneDDS <pre><code>// Set to Keep_all, but limited to 1 due to max_samples_per_instance\n/* Check if resource max_samples QoS exceeded */ \nif (rhc-&gt;reader &amp;&amp; rhc-&gt;max_samples != DDS_LENGTH_UNLIMITED &amp;&amp; rhc-&gt;n_vsamples &gt;= (uint32_t) rhc-&gt;max_samples) \n{ \ncb_data-&gt;raw_status_id = (int) DDS_SAMPLE_REJECTED_STATUS_ID; \ncb_data-&gt;extra = DDS_REJECTED_BY_SAMPLES_LIMIT; \ncb_data-&gt;handle = inst-&gt;iid; \ncb_data-&gt;add = true; return false;  \n} \n/* Check if resource max_samples_per_instance QoS exceeded */ \nif (rhc-&gt;reader &amp;&amp; rhc-&gt;max_samples_per_instance != DDS_LENGTH_UNLIMITED &amp;&amp; inst-&gt;nvsamples &gt;= (uint32_t) rhc-&gt;max_samples_per_instance) \n{ \ncb_data-&gt;raw_status_id = (int) DDS_SAMPLE_REJECTED_STATUS_ID; \ncb_data-&gt;extra = DDS_REJECTED_BY_SAMPLES_PER_INSTANCE_LIMIT; \ncb_data-&gt;handle = inst-&gt;iid; \ncb_data-&gt;add = true; return false; \n}\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-10","title":"Rule 10","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// Changing the owner in deadline_missed()\nvoid deadline_missed() \n{ if (fastdds::rtps::c_Guid_Unknown != current_owner.first) \n{ if (alive_writers.remove_if([&amp;](const WriterOwnership&amp; item) \n{ return item.first == current_owner.first; })) \n{ \ncurrent_owner.second = 0;\ncurrent_owner.first = fastdds::rtps::c_Guid_Unknown; \nif (alive_writers.empty() &amp;&amp; (InstanceStateKind::ALIVE_INSTANCE_STATE == instance_state)) \n{ instance_state = InstanceStateKind::NOT_ALIVE_NO_WRITERS_INSTANCE_STATE; } \nif (ALIVE_INSTANCE_STATE == instance_state) \n{ update_owner(); } } } }\n\n// However, assume that the `deadline_missed()` function is not infinite!\nbool DataReaderImpl::deadline_timer_reschedule()\n{\nassert(qos_.deadline().period != dds::c_TimeInfinite);\n</code></pre></li> <li>RMW/Implementation: CycloneDDS <pre><code>// When a deadline is missed, inst-&gt;wr_iid_islive is reset to 0, thereby changing ownership. \n// However, if the deadline is set to infinity, inst-&gt;wr_iid_islive will never be reset to 0.\n#endif /* DDS_HAS_LIFESPAN */\n#ifdef DDS_HAS_DEADLINE_MISSED\nddsrt_mtime_t dds_rhc_default_deadline_missed_cb(void *hc, ddsrt_mtime_t tnow)\n{ struct dds_rhc_default *rhc = hc; \nvoid *vinst; \nddsrt_mtime_t tnext; \nddsrt_mutex_lock (&amp;rhc-&gt;lock); \nwhile ((tnext = deadline_next_missed_locked (&amp;rhc-&gt;deadline, tnow, &amp;vinst)).v == 0) \n{ struct rhc_instance *inst = vinst; deadline_reregister_instance_locked (&amp;rhc-&gt;deadline, &amp;inst-&gt;deadline, tnow); \ninst-&gt;wr_iid_islive = 0; \nddsi_status_cb_data_t cb_data; \ncb_data.raw_status_id = (int) DDS_REQUESTED_DEADLINE_MISSED_STATUS_ID; \ncb_data.extra = 0; \ncb_data.handle = inst-&gt;iid; \ncb_data.add = true; \nddsrt_mutex_unlock (&amp;rhc-&gt;lock); \ndds_reader_status_cb (&amp;rhc-&gt;reader-&gt;m_entity, &amp;cb_data); ddsrt_mutex_lock (&amp;rhc-&gt;lock); \ntnow = ddsrt_time_monotonic (); } \nddsrt_mutex_unlock (&amp;rhc-&gt;lock); \nreturn tnext;}\n#endif /* DDS_HAS_DEADLINE_MISSED */\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-11","title":"Rule 11","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// If the lease duration is infinite, the writer is not registered. Consequently, lease expiry \u2192 not alive does not occur \u2192 owner change fails.\nif (liveliness_lease_duration_ &lt; c_TimeInfinite)\n{\nauto wlp = this-&gt;mp_RTPSParticipant-&gt;wlp();\nif ( wlp != nullptr)\n{ \nwlp-&gt;sub_liveliness_manager_-&gt;add_writer(\nwdata.guid(),\nliveliness_kind_,\nliveliness_lease_duration_);\n}\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-12","title":"Rule 12","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// livelinss lost switched to NOT_ALIVE\n// However, if set to infinite, the transition does not occur, so the sample cannot be automatically removed.\nbool LivelinessManager::timer_expired()\n{ std::unique_lock&lt;std::mutex&gt; lock(mutex_); \nif (timer_owner_ == nullptr) \n{ EPROSIMA_LOG_ERROR(RTPS_WRITER, \"Liveliness timer expired but there is no writer\"); \nreturn false; } \nelse { timer_owner_-&gt;status = LivelinessData::WriterStatus::NOT_ALIVE; }\n</code></pre></li> <li>RMW/Implementation: CycloneDDS <pre><code>// Upon expiry of the lease term,\ncase DDSI_EK_PROXY_WRITER:\nddsi_proxy_writer_set_notalive ((struct ddsi_proxy_writer *) l-&gt;entity, true);\nbreak;\ncase DDSI_EK_WRITER:\nddsi_writer_set_notalive ((struct ddsi_writer *) l-&gt;entity, true);\nbreak;\n// However, if set to infinite, it will not expire, so the data deletion function will not operate.\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-13","title":"Rule 13","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// Stored in permanent storage\nvoid StatefulPersistentReader::set_last_notified(\nconst GUID_t&amp; writer_guid,\nconst SequenceNumber_t&amp; seq)\n{\nhistory_state_-&gt;history_record[writer_guid] = seq;\npersistence_-&gt;update_writer_seq_on_storage(persistence_guid_, writer_guid, seq);\n}\n// But the data vanished in an instant.\nDuration_t autopurge_no_writer_samples_delay;\n/**\n* @brief Indicates the duration the DataReader must retain information regarding instances that have the\n* instance_state NOT_ALIVE_DISPOSED. &lt;br&gt;\n* By default, c_TimeInfinite.\n*/\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-14","title":"Rule 14","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// Partition change \u2192 Unmatching \u2192 Call ddsi_rhc_unregister_wr (rd-&gt;rhc, &amp;wrinfo);\n// \u21d2 Instance is immediately unregistered from the deadline. \u2192 Deadline timer is not updated.\nif (inst-&gt;deadline_reg)\n{\ninst-&gt;deadline_reg = 0;\ndeadline_unregister_instance_locked (&amp;rhc-&gt;deadline, &amp;inst-&gt;deadline);\n}\n// During the process of dynamically changing partitions, instances are removed from the deadline list due to writer unregistration.\n// Since the timer is not reset to the \"next expiry time\" upon unregistration, the deadline timer ceases to function once the list becomes empty.\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-15","title":"Rule 15","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// The writer signals liveliness, but if a reader becomes involved in the partition dynamic change process, it no longer receives updates to the writer's liveliness\n// \u2192 Subsequently, when the lease expires, the notification is sent only to readers that are \"still matched\".\n// However, if there are no readers, the liveliness notification is disabled even during the partition dynamic change process.\nstatic void proxy_writer_notify_liveliness_change_may_unlock (struct ddsi_proxy_writer *pwr)\n{ \nstruct ddsi_alive_state alive_state; \nproxy_writer_get_alive_state_locked (pwr, &amp;alive_state); \nstruct ddsi_guid rdguid; \nstruct ddsi_pwr_rd_match *m; memset (&amp;rdguid, 0, sizeof (rdguid)); \nwhile (pwr-&gt;alive_vclock == alive_state.vclock &amp;&amp;\u00a0(m = ddsrt_avl_lookup_succ (&amp;ddsi_pwr_readers_treedef, &amp;pwr-&gt;readers, &amp;rdguid)) != NULL) \n{ rdguid = m-&gt;rd_guid; ddsrt_mutex_unlock (&amp;pwr-&gt;e.lock); \n/* unlocking pwr means alive state may have changed already; we break out of the loop once we\u00a0detect this but there for the reader in the current iteration, anything is possible */ \nddsi_reader_update_notify_pwr_alive_state_guid (&amp;rdguid, pwr, &amp;alive_state); \nddsrt_mutex_lock (&amp;pwr-&gt;e.lock); }}\n// \u2192 The recipients of the notification are only the readers listed in pwr\u2192readers.\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-16","title":"Rule 16","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// The system is designed so that not just anyone can delete data;\n// only the sender with the most powerful permissions can alter the status.\nbool writer_dispose\n( const fastrtps::rtps::GUID_t&amp; writer_guid, const uint32_t ownership_strength) \n{ bool ret_val = false; writer_set(writer_guid, ownership_strength); \nif (ownership_strength &gt;= current_owner.second) \n{ current_owner.first = writer_guid; current_owner.second = ownership_strength; \nif (InstanceStateKind::ALIVE_INSTANCE_STATE == instance_state) \n{ ret_val = true; instance_state = InstanceStateKind::NOT_ALIVE_DISPOSED_INSTANCE_STATE; } } \nreturn ret_val; }\n</code></pre></li> <li>RMW/Implementation: CycloneDDS <pre><code>// Only writers with a stronger strength can now modify the instance's state.\nstatic int inst_accepts_sample (const struct dds_rhc_default *rhc, const struct rhc_instance *inst, const struct ddsi_writer_info *wrinfo, const struct ddsi_serdata sample, const bool has_data)\n{if (rhc-&gt;by_source_ordering){\nif (sample-&gt;timestamp.v &gt; inst-&gt;tstamp.v)\n{/ ok /}\nelse if (sample-&gt;timestamp.v &lt; inst-&gt;tstamp.v)\n{return 0;}\nelse if (inst_accepts_sample_by_writer_guid (inst, wrinfo))\n{/ ok /}\nelse\n{return 0;}}\nif (rhc-&gt;exclusive_ownership &amp;&amp; inst-&gt;wr_iid_islive &amp;&amp; inst-&gt;wr_iid != wrinfo-&gt;iid)\n{int32_t strength = wrinfo-&gt;ownership_strength;\nif (strength &gt; inst-&gt;strength) {\n/ ok /\n} else if (strength &lt; inst-&gt;strength) {\nreturn 0;\n} else if (inst_accepts_sample_by_writer_guid (inst, wrinfo)) {\n/ ok */\n} else {\nreturn 0;}}\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-17","title":"Rule 17","text":"<ul> <li>RMW/Implementation:FastDDS <pre><code>// Keep_last removes the oldest sample when the depth is full.\n {// Try to substitute the oldest sample.\nCacheChange_t* first_change = instance_changes.at(0);\nif (a_change-&gt;sourceTimestamp &gt;= first_change-&gt;sourceTimestamp){\n// As the instance is ordered by source timestamp, we can always remove the first one.\nret_value = remove_change_sub(first_change);}\nelse{\n// Received change is older than oldest, and should be discarded\nreturn true;}}\n\n// Looking at the lifespan timer code, it states that it \"may already have been removed from the history\".\nCacheChange_t* earliest_change; \nwhile (history_.get_earliest_change(&amp;earliest_change)) \n{ fastdds::rtps::Time_t expiration_ts = earliest_change-&gt;sourceTimestamp + qos_.lifespan().duration; \n// Check that the earliest change has expired (the change which started the timer could have been removed from the history) \nif (current_ts &lt; expiration_ts) \n{ fastdds::rtps::Time_t interval = expiration_ts - current_ts; lifespan_timer_-&gt;update_interval_millisec(interval.to_ns() * 1e-6); return true; } \n// The earliest change has expired \nhistory_.remove_change_sub(earliest_change); \ntry_notify_read_conditions(); } \nreturn false;\n</code></pre></li> <li>RMW/Implementation:CycloneDDS <pre><code>// A situation where a sample is overwritten and disappears due to history depth before it \"expires\" based on its lifespan.\n// In the code, it deletes data using keep_last and then deletes it again based on lifespan.\nif (inst-&gt;nvsamples == rhc-&gt;history_depth) { \n/* replace oldest sample; latest points to the latest one, the\u00a0list is circular from old -&gt; new, so latest-&gt;next is the oldest */ \ninst_clear_invsample_if_exists (rhc, inst, trig_qc); \nassert (inst-&gt;latest != NULL); s = inst-&gt;latest-&gt;next; \nassert (trig_qc-&gt;dec_conds_sample == 0); \nddsi_serdata_unref (s-&gt;sample);\n#ifdef DDS_HAS_LIFESPAN \nlifespan_unregister_sample_locked (&amp;rhc-&gt;lifespan, &amp;s-&gt;lifespan);\n#endif \ntrig_qc-&gt;dec_sample_read = s-&gt;isread; \ntrig_qc-&gt;dec_conds_sample = s-&gt;conds; \nif (s-&gt;isread) \n{ inst-&gt;nvread--; rhc-&gt;n_vread--; } }\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-18","title":"Rule 18","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// Keep_All waits until an ACK is received confirming that the oldest data has been successfully delivered to the other party before deleting the data.\nelse if (history_qos_.kind == KEEP_ALL_HISTORY_QOS) \n{ if (vit-&gt;second.cache_changes.size() &lt; static_cast&lt;size_t&gt;(resource_limited_qos_.max_samples_per_instance)) \n{ add = true; } \nelse { \nSequenceNumber_t seq_to_remove = vit-&gt;second.cache_changes.front()-&gt;sequenceNumber; \nif (!mp_writer-&gt;wait_for_acknowledgement(seq_to_remove, max_blocking_time, lock)) { \n// Timeout waiting. Will not add change to history. \nbreak; } \n// vit may have been invalidated \nif (!find_or_add_key(change-&gt;instanceHandle, change-&gt;serializedPayload, &amp;vit)) \n{ break; } \n// If the change we were trying to remove was already removed, try again \nif (vit-&gt;second.cache_changes.empty() || vit-&gt;second.cache_changes.front()-&gt;sequenceNumber != seq_to_remove) \n{ continue; } \n// Remove change if still present \nadd = remove_change_pub(vit-&gt;second.cache_changes.front()); } }\n\n// Looking at the lifespan timer code, it states that it \"may already have been removed from the history\".\nCacheChange_t* earliest_change; \nwhile (history_.get_earliest_change(&amp;earliest_change)) \n{ fastdds::rtps::Time_t expiration_ts = earliest_change-&gt;sourceTimestamp + qos_.lifespan().duration; \n// Check that the earliest change has expired (the change which started the timer could have been removed from the history) \nif (current_ts &lt; expiration_ts) \n{ fastdds::rtps::Time_t interval = expiration_ts - current_ts; lifespan_timer_-&gt;update_interval_millisec(interval.to_ns() * 1e-6); return true; } \n// The earliest change has expired \nhistory_.remove_change_sub(earliest_change); \ntry_notify_read_conditions(); } \nreturn false;\n</code></pre></li> <li>RMW/Implementation: CycloneDDS <pre><code>// Although set to Keep_all, due to max_samples_per_instance, samples are not saved at all before their lifespan expires, meaning no samples are deleted by lifespan.\n/* Check if resource max_samples QoS exceeded */ \nif (rhc-&gt;reader &amp;&amp; rhc-&gt;max_samples != DDS_LENGTH_UNLIMITED &amp;&amp; rhc-&gt;n_vsamples &gt;= (uint32_t) rhc-&gt;max_samples) \n{ \ncb_data-&gt;raw_status_id = (int) DDS_SAMPLE_REJECTED_STATUS_ID; \ncb_data-&gt;extra = DDS_REJECTED_BY_SAMPLES_LIMIT; \ncb_data-&gt;handle = inst-&gt;iid; \ncb_data-&gt;add = true; return false; \n} \n/* Check if resource max_samples_per_instance QoS exceeded */ \nif (rhc-&gt;reader &amp;&amp; rhc-&gt;max_samples_per_instance != DDS_LENGTH_UNLIMITED &amp;&amp; inst-&gt;nvsamples &gt;= (uint32_t) rhc-&gt;max_samples_per_instance) \n{ \ncb_data-&gt;raw_status_id = (int) DDS_SAMPLE_REJECTED_STATUS_ID; \ncb_data-&gt;extra = DDS_REJECTED_BY_SAMPLES_PER_INSTANCE_LIMIT; \ncb_data-&gt;handle = inst-&gt;iid; \ncb_data-&gt;add = true; return false; \n}\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-19","title":"Rule 19","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// autoenable_created_entities = FALSE means entities are not automatically enabled.\npublisher_-&gt;rtps_participant()-&gt;register_writer(writer_, topic_desc, wqos);\n// \u21d2 Only within enable() are register_writer() / register_reader() called to register with discovery.\n// As VOLATILE entities do not become existing until enabled, any previously sent data is discarded.\n// When set to Volatile, upon enabling, all previous data will not be received. \n</code></pre></li> <li>RMW/Implementation: CycloneDDS <pre><code>// When set to VOLATILE, previous data is not fully retrieved.\n// The implementation where autoenable_created_entities=FALSE is not present.\nif(qos-&gt;durability.kind == DDS_DURABILITY_VOLATILE) \n{ opts.historyCapacity = 0; } \nelse \n{ \n// Transient Local and stronger  \nif (qos-&gt;durability_service.history.kind == DDS_HISTORY_KEEP_LAST) \n{ \nopts.historyCapacity = (uint64_t)qos-&gt;durability_service.history.depth;\n } \nelse { opts.historyCapacity = 0; } }\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-20","title":"Rule 20","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// Upon successful matching, it may be treated as a later-join, potentially resulting in duplicate data being received.\nif (!matched) //Different partitions\n{\nEPROSIMA_LOG_WARNING(RTPS_EDP, \"INCOMPATIBLE QOS (topic: \" &lt;&lt; rdata-&gt;topic_name &lt;&lt; \"): Different Partitions\");\nreason.set(MatchingFailureMask::partitions);\n}\n// transient_local always enters PRMSS_SYNC upon (re)matching and does not verify whether it has already received the entire data\n// \u2192 Consequently, even if the partition is rematched, it is treated as a new match, leading to redundant data reception.\nif (rd-&gt;handle_as_transient_local)\nm-&gt;in_sync = PRMSS_OUT_OF_SYNC;\nelse if (vendor_is_eclipse (pwr-&gt;c.vendor))\nm-&gt;in_sync = PRMSS_OUT_OF_SYNC;\nelse\nm-&gt;in_sync = PRMSS_SYNC;\nm-&gt;u.not_in_sync.end_of_tl_seq = MAX_SEQ_NUMBER;\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-28-rule-29-rule-30","title":"Rule 28 / Rule 29 / Rule 30","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// If autodispose_unregistered_instances=false and the application does not explicitly call dispose(), the DataWriter will not send a disposal notification (DISPOSE), meaning the NOT_ALIVE_DISPOSED state is not passed to the DataReader.\n// Consequently, the autopurge_disposed_samples_delay setting in READER_DATA_LIFECYCLE becomes irrelevant for that instance, rendering it meaningless.\n/**\n* @brief Indicates the duration the DataReader must retain information regarding instances that have the\n* instance_state NOT_ALIVE_DISPOSED. &lt;br&gt;\n* By default, dds::c_TimeInfinite.\n*/\ndds::Duration_t autopurge_disposed_samples_delay;\n</code></pre></li> <li>RMW/Implementation: CycloneDDS <pre><code>// As the instance is not in the DISPOSED state, autopurge_disposed_samples_delay does not apply to it in the first place.\nvoid ddsi_reader_update_notify_pwr_alive_state (struct ddsi_reader *rd, const struct ddsi_proxy_writer *pwr, const struct ddsi_alive_state *alive_state)\n{ struct ddsi_rd_pwr_match *m; bool notify = false; \nint delta = 0; \n/* -1: alive -&gt; not_alive; 0: unchanged; 1: not_alive -&gt; alive */ \nddsrt_mutex_lock (&amp;rd-&gt;e.lock); \nif ((m = ddsrt_avl_lookup (&amp;ddsi_rd_writers_treedef, &amp;rd-&gt;writers, &amp;pwr-&gt;e.guid)) != NULL) \n{ if ((int32_t) (alive_state-&gt;vclock - m-&gt;pwr_alive_vclock) &gt; 0) \n{ delta = (int) alive_state-&gt;alive - (int) m-&gt;pwr_alive; notify = true; \nm-&gt;pwr_alive = alive_state-&gt;alive; \nm-&gt;pwr_alive_vclock = alive_state-&gt;vclock; } } \nddsrt_mutex_unlock (&amp;rd-&gt;e.lock); \nif (delta &lt; 0 &amp;&amp; rd-&gt;rhc) { struct ddsi_writer_info wrinfo; \nddsi_make_writer_info (&amp;wrinfo, &amp;pwr-&gt;e, pwr-&gt;c.xqos, NN_STATUSINFO_UNREGISTER); ddsi_rhc_unregister_wr (rd-&gt;rhc, &amp;wrinfo); }\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-34","title":"Rule 34","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// Set to best-effort mode, so NOT_ALIVE_DISPOSED_UNREGISTERED messages may be lost.\nf !defined(NDEBUG) \nif (history_.is_key_registered(ih)) \n{ WriteParams wparams; ChangeKind_t change_kind = NOT_ALIVE_DISPOSED; \nif (!dispose) \n{ change_kind = qos_.writer_data_lifecycle().autodispose_unregistered_instances ? NOT_ALIVE_DISPOSED_UNREGISTERED : \nNOT_ALIVE_UNREGISTERED; }\n</code></pre></li> <li>RMW/Implementation: CycloneDDS <pre><code>// There is absolutely no retransmission/ACK-based guarantee for the transmitted sample (data + unregister/dispose control message).\nstatic int rhc_unregister_updateinst ( \u2026.\n if (!inst-&gt;isdisposed)\n{\nif (inst-&gt;latest == NULL || inst-&gt;latest-&gt;isread)\n{\ninst_set_invsample (rhc, inst, trig_qc, nda);\nupdate_inst_no_wr_iid (inst, wrinfo, tstamp);\n}\nif (!inst-&gt;autodispose)\nrhc-&gt;n_not_alive_no_writers++;\n// When the Writer sends an unregister request, the Reader's RHC receives it and, if auto_dispose == 1, transitions the instance to the DISPOSED state.\n</code></pre></li> </ul>"},{"location":"rules/IMP/#rule-40","title":"Rule 40","text":"<ul> <li>RMW/Implementation: FastDDS <pre><code>// 1. Writer previously issued multiple samples (e.g., one hour ago)\n// 2. Reader joins late and receives past samples as TRANSIENT_LOCAL\n// 3. Each retransmitted sample calls on_new_cache_change_added\n// 4. Deadline timer is reset with each retransmission\nbool DataReaderImpl::on_new_cache_change_added( \nconst CacheChange_t* const change)\n{ std::lock_guard&lt;RecursiveTimedMutex&gt; guard(reader_-&gt;getMutex()); \nif (qos_.deadline().period != c_TimeInfinite) \n{ if (!history_.set_next_deadline( change-&gt;instanceHandle, steady_clock::now() + duration_cast&lt;system_clock::duration&gt;(deadline_duration_us_))) \n{ logError(SUBSCRIBER, \"Could not set next deadline in the history\"); } \nelse if (timer_owner_ == change-&gt;instanceHandle || timer_owner_ == InstanceHandle_t()) \n{ if (deadline_timer_reschedule()) \n{ deadline_timer_-&gt;cancel_timer(); deadline_timer_-&gt;restart_timer(); } } }\n</code></pre></li> <li>RMW/Implementation: CycloneDDS <pre><code>The deadline is set based on the sample reception time\n// \u2192 With the transient setting, previous data arrives, causing the sample reception time to reset the deadline.\nstatic void postprocess_instance_update (struct dds_rhc_default * __restrict rhc, struct rhc_instance * __restrict * __restrict instptr, const struct trigger_info_pre *pre, const struct trigger_info_post *post, struct trigger_info_qcond *trig_qc)\n{ { struct rhc_instance *inst = *instptr;\n#ifdef DDS_HAS_DEADLINE_MISSED \nif (inst-&gt;isdisposed) \n{ if (inst-&gt;deadline_reg) \n{ inst-&gt;deadline_reg = 0; \ndeadline_unregister_instance_locked (&amp;rhc-&gt;deadline, &amp;inst-&gt;deadline); } } \nelse \n{ if (inst-&gt;deadline_reg) \ndeadline_renew_instance_locked (&amp;rhc-&gt;deadline, &amp;inst-&gt;deadline); \nelse { deadline_register_instance_locked (&amp;rhc-&gt;deadline, &amp;inst-&gt;deadline, ddsrt_time_monotonic ()); \ninst-&gt;deadline_reg = 1; } }\n#endif\n</code></pre></li> </ul>"},{"location":"rules/STD/","title":"STD Rules","text":"<p>This page describes the QoS dependency and consistency rules derived from the OMG DDS and ROS 2 Standard specifications. Violation of these rules typically results in entity creation failure or immediate communication incompatibility.</p>"},{"location":"rules/STD/#stage-1","title":"Stage 1","text":"<p>Intra-entity Dependency Validation</p> 1 HIST \u2194 RESLIM Structural        [HIST.kind = KEEP_LAST] \u2227 [HIST.depth &gt; RESLIM.mpi]      Entity Pub, Sub Basis STD 2 RESLIM \u2194 RESLIM Structural        [max_samples &lt; max_samples_per_instance]      Entity Pub, Sub Basis STD"},{"location":"rules/STD/#stage-2","title":"Stage 2","text":"<p>Inter-entity Dependency Validation</p> 21 PART \u2194 PART Structural        [Writer.PART \u2229 Reader.PART] = \u2205      Entity Pub \u2194 Sub Basis STD 22 RELIAB \u2194 RELIAB Structural        [Writer.RELIAB &lt; Reader.RELIAB]      Entity Pub \u2194 Sub Basis STD 23 DURABL \u2194 DURABL Structural        [Writer.DURABL &lt; Reader.DURABL]      Entity Pub \u2194 Sub Basis STD 24 DEADLN \u2194 DEADLN Structural        [Writer.DEADLN.period &gt; Reader.DEADLN.period]      Entity Pub \u2194 Sub Basis STD 25 LIVENS \u2194 LIVENS Structural        [W.LIVENS &lt; R.LIVENS] \u2228 [W.lease &gt; R.lease]      Entity Pub \u2194 Sub Basis STD 26 OWNST \u2194 OWNST Structural        [Writer.OWNST \u2260 Reader.OWNST]      Entity Pub \u2194 Sub Basis STD 27 DESTORD \u2194 DESTORD Structural        [Writer.DESTORD &lt; Reader.DESTORD]      Entity Pub \u2194 Sub Basis STD"}]}